"""
–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ–≥—Ä–∞—Ç–æ—Ä –¥–ª—è car.v2i.multiclass –∏ Car damages.v3i.multiclass
–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç CSV –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∏ —Å–æ–∑–¥–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—É—é multiclass —Å—Ç—Ä—É–∫—Ç—É—Ä—É
"""
import os
import pandas as pd
import shutil
from pathlib import Path
import json
from collections import defaultdict
from PIL import Image
import numpy as np

class NewDatasetIntegrator:
    def __init__(self):
        self.dataset_paths = {
            "car_v2": r"C:\Users\–î–∏–º–∞—à\Desktop\python\hackaton\data\car.v2i.multiclass",
            "car_damages_v3": r"C:\Users\–î–∏–º–∞—à\Desktop\python\hackaton\data\Car damages.v3i.multiclass"
        }
        
        self.output_base = r"C:\Users\–î–∏–º–∞—à\Desktop\python\hackaton\data"
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –º–∞–ø–ø–∏–Ω–≥–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        self.mapping_strategies = {
            "car_v2": {
                "csv_columns": ["filename", "bus", "car", "repair-car", "truck"],
                "class_mapping": {
                    # –û—Å–Ω–æ–≤—ã–≤–∞–µ–º—Å—è –Ω–∞ one-hot encoding –≤ CSV
                    "car": 0,           # no_damage (—á–∏—Å—Ç—ã–µ –º–∞—à–∏–Ω—ã)
                    "repair-car": 1,    # minor_damage (–º–∞—à–∏–Ω—ã –≤ —Ä–µ–º–æ–Ω—Ç–µ)
                    "bus": 1,           # minor_damage (–∞–≤—Ç–æ–±—É—Å—ã —Å—á–∏—Ç–∞–µ–º –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–º–∏)
                    "truck": 1          # minor_damage (–≥—Ä—É–∑–æ–≤–∏–∫–∏ —Å—á–∏—Ç–∞–µ–º –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–º–∏)
                },
                "description": "–î–∞—Ç–∞—Å–µ—Ç —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∞ —Å one-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"
            },
            
            "car_damages_v3": {
                "csv_columns": ["filename", "dent", "good_condition", "scratch", "severe damage"],
                "class_mapping": {
                    "good_condition": 0,    # no_damage
                    "scratch": 1,           # minor_damage  
                    "dent": 1,              # minor_damage
                    "severe damage": 2      # major_damage
                },
                "description": "–î–∞—Ç–∞—Å–µ—Ç –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π"
            }
        }
        
        self.class_names = ["no_damage", "minor_damage", "major_damage"]
        
    def analyze_csv_annotations(self, dataset_key: str):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç CSV –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        
        dataset_path = self.dataset_paths[dataset_key]
        strategy = self.mapping_strategies[dataset_key]
        
        print(f"\nüîç –ê–ù–ê–õ–ò–ó CSV –ê–ù–ù–û–¢–ê–¶–ò–ô: {dataset_key}")
        print("="*50)
        
        analysis = {
            "total_samples": 0,
            "splits": {},
            "class_distribution": defaultdict(int),
            "samples_per_split": {}
        }
        
        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º split'–∞–º (train/test/valid)
        for split in ["train", "test", "valid"]:
            split_path = os.path.join(dataset_path, split)
            csv_path = os.path.join(split_path, "_classes.csv")
            
            if not os.path.exists(csv_path):
                continue
                
            print(f"\nüìä –ê–Ω–∞–ª–∏–∑ {split}:")
            
            df = pd.read_csv(csv_path)
            print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")
            print(f"   –ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º one-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
            split_distribution = defaultdict(int)
            
            for idx, row in df.iterrows():
                filename = row['filename']
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª–∞—Å—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ one-hot encoding
                predicted_class = self.predict_class_from_csv(row, strategy)
                
                if predicted_class is not None:
                    split_distribution[predicted_class] += 1
                    analysis["class_distribution"][predicted_class] += 1
                    analysis["total_samples"] += 1
            
            analysis["splits"][split] = dict(split_distribution)
            analysis["samples_per_split"][split] = len(df)
            
            print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
            for class_idx, count in split_distribution.items():
                class_name = self.class_names[class_idx]
                percentage = (count / len(df)) * 100 if len(df) > 0 else 0
                print(f"     {class_name}: {count} ({percentage:.1f}%)")
        
        print(f"\nüìà –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê {dataset_key}:")
        print(f"   –í—Å–µ–≥–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {analysis['total_samples']}")
        
        if analysis["total_samples"] > 0:
            for class_idx, count in analysis["class_distribution"].items():
                class_name = self.class_names[class_idx]
                percentage = (count / analysis["total_samples"]) * 100
                print(f"   {class_name}: {count} ({percentage:.1f}%)")
        
        return analysis
    
    def predict_class_from_csv(self, row, strategy):
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–ª–∞—Å—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ one-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ CSV"""
        
        class_mapping = strategy["class_mapping"]
        
        # –ò—â–µ–º –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–∑–Ω–∞—á–µ–Ω–∏–µ 1)
        for col_name, class_idx in class_mapping.items():
            if col_name in row and row[col_name] == 1:
                return class_idx
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None
        return None
    
    def create_integrated_dataset(self):
        """–°–æ–∑–¥–∞–µ—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        
        print(f"\nüöÄ –°–û–ó–î–ê–ù–ò–ï –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–û–ì–û –î–ê–¢–ê–°–ï–¢–ê")
        print("="*60)
        
        output_path = os.path.join(self.output_base, "integrated_multiclass_dataset")
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫
        for split in ["train", "test", "valid"]:
            for class_idx, class_name in enumerate(self.class_names):
                class_dir = os.path.join(output_path, split, class_name)
                os.makedirs(class_dir, exist_ok=True)
        
        total_stats = {
            "datasets_processed": 0,
            "total_images": 0,
            "class_distribution": defaultdict(int),
            "split_distribution": defaultdict(int),
            "errors": []
        }
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        for dataset_key in self.dataset_paths.keys():
            print(f"\nüìÅ –û–ë–†–ê–ë–û–¢–ö–ê –î–ê–¢–ê–°–ï–¢–ê: {dataset_key}")
            
            try:
                dataset_stats = self.process_single_dataset(dataset_key, output_path)
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                total_stats["datasets_processed"] += 1
                total_stats["total_images"] += dataset_stats["images_processed"]
                
                for class_idx, count in dataset_stats["class_distribution"].items():
                    total_stats["class_distribution"][class_idx] += count
                
                for split, count in dataset_stats["split_distribution"].items():
                    total_stats["split_distribution"][split] += count
                    
            except Exception as e:
                error_msg = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {dataset_key}: {e}"
                total_stats["errors"].append(error_msg)
                print(f"‚ùå {error_msg}")
        
        # –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.print_integration_summary(total_stats, output_path)
        
        # –°–æ–∑–¥–∞–µ–º CSV —Å –º–µ—Ç–∞-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        self.create_integration_metadata(total_stats, output_path)
        
        return output_path, total_stats
    
    def process_single_dataset(self, dataset_key: str, output_path: str):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –¥–∞—Ç–∞—Å–µ—Ç"""
        
        dataset_path = self.dataset_paths[dataset_key]
        strategy = self.mapping_strategies[dataset_key]
        
        stats = {
            "images_processed": 0,
            "images_skipped": 0,
            "class_distribution": defaultdict(int),
            "split_distribution": defaultdict(int)
        }
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π split
        for split in ["train", "test", "valid"]:
            split_path = os.path.join(dataset_path, split)
            csv_path = os.path.join(split_path, "_classes.csv")
            
            if not os.path.exists(csv_path):
                continue
            
            print(f"   üìÇ –û–±—Ä–∞–±–æ—Ç–∫–∞ {split}...")
            
            df = pd.read_csv(csv_path)
            
            for idx, row in df.iterrows():
                filename = row['filename']
                source_image_path = os.path.join(split_path, filename)
                
                if not os.path.exists(source_image_path):
                    stats["images_skipped"] += 1
                    continue
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª–∞—Å—Å
                predicted_class = self.predict_class_from_csv(row, strategy)
                
                if predicted_class is None:
                    stats["images_skipped"] += 1
                    continue
                
                # –ö–æ–ø–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –ø–∞–ø–∫—É
                class_name = self.class_names[predicted_class]
                target_dir = os.path.join(output_path, split, class_name)
                
                # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
                base_name, ext = os.path.splitext(filename)
                unique_filename = f"{dataset_key}_{base_name}{ext}"
                target_path = os.path.join(target_dir, unique_filename)
                
                try:
                    shutil.copy2(source_image_path, target_path)
                    
                    stats["images_processed"] += 1
                    stats["class_distribution"][predicted_class] += 1
                    stats["split_distribution"][split] += 1
                    
                except Exception as e:
                    print(f"      ‚ùå –û—à–∏–±–∫–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è {filename}: {e}")
                    stats["images_skipped"] += 1
            
            processed = stats["split_distribution"][split]
            print(f"      ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        
        return stats
    
    def print_integration_summary(self, stats, output_path):
        """–í—ã–≤–æ–¥–∏—Ç –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        
        print(f"\nüéâ –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò–ù–¢–ï–ì–†–ê–¶–ò–ò")
        print("="*60)
        print(f"üìÅ –í—ã—Ö–æ–¥–Ω–∞—è –ø–∞–ø–∫–∞: {output_path}")
        print(f"üìä –î–∞—Ç–∞—Å–µ—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['datasets_processed']}")
        print(f"üñºÔ∏è –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {stats['total_images']}")
        
        if stats["errors"]:
            print(f"‚ùå –û—à–∏–±–∫–∏ ({len(stats['errors'])}):")
            for error in stats["errors"]:
                print(f"   {error}")
        
        print(f"\nüìà –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –ö–õ–ê–°–°–ê–ú:")
        total = sum(stats["class_distribution"].values())
        
        for class_idx, count in stats["class_distribution"].items():
            class_name = self.class_names[class_idx]
            percentage = (count / total) * 100 if total > 0 else 0
            print(f"   {class_name}: {count} ({percentage:.1f}%)")
        
        print(f"\nüìã –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û SPLIT'–ê–ú:")
        for split, count in stats["split_distribution"].items():
            percentage = (count / total) * 100 if total > 0 else 0
            print(f"   {split}: {count} ({percentage:.1f}%)")
    
    def create_integration_metadata(self, stats, output_path):
        """–°–æ–∑–¥–∞–µ—Ç –º–µ—Ç–∞-–¥–∞–Ω–Ω—ã–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        
        metadata = {
            "integration_date": "2025-09-13",
            "source_datasets": self.dataset_paths,
            "mapping_strategies": self.mapping_strategies,
            "statistics": {
                "total_images": stats["total_images"],
                "datasets_processed": stats["datasets_processed"],
                "class_distribution": dict(stats["class_distribution"]),
                "split_distribution": dict(stats["split_distribution"]),
                "errors": stats["errors"]
            },
            "class_names": self.class_names
        }
        
        metadata_path = os.path.join(output_path, "integration_metadata.json")
        
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_path}")
    
    def validate_integrated_dataset(self, output_path):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        
        print(f"\nüîç –í–ê–õ–ò–î–ê–¶–ò–Ø –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–û–ì–û –î–ê–¢–ê–°–ï–¢–ê")
        print("="*60)
        
        validation_results = {
            "structure_valid": True,
            "image_counts": {},
            "corrupted_images": [],
            "missing_classes": []
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫
        for split in ["train", "test", "valid"]:
            validation_results["image_counts"][split] = {}
            
            for class_idx, class_name in enumerate(self.class_names):
                class_dir = os.path.join(output_path, split, class_name)
                
                if not os.path.exists(class_dir):
                    validation_results["missing_classes"].append(f"{split}/{class_name}")
                    validation_results["structure_valid"] = False
                    continue
                
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                image_files = [f for f in os.listdir(class_dir) 
                             if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
                
                validation_results["image_counts"][split][class_name] = len(image_files)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å
                for img_file in image_files[:3]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 3
                    img_path = os.path.join(class_dir, img_file)
                    try:
                        with Image.open(img_path) as img:
                            img.verify()
                    except Exception as e:
                        validation_results["corrupted_images"].append(f"{split}/{class_name}/{img_file}")
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        print(f"‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–∞–ø–æ–∫: {'OK' if validation_results['structure_valid'] else '–û–®–ò–ë–ö–ê'}")
        
        if validation_results["missing_classes"]:
            print(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∫–ª–∞—Å—Å—ã: {validation_results['missing_classes']}")
        
        if validation_results["corrupted_images"]:
            print(f"‚ùå –ü–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {len(validation_results['corrupted_images'])}")
            for corrupted in validation_results["corrupted_images"][:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
                print(f"   {corrupted}")
        else:
            print(f"‚úÖ –í—Å–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ü–µ–ª—ã–µ")
        
        # –í—ã–≤–æ–¥–∏–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Å—á–µ—Ç—á–∏–∫–∏
        print(f"\nüìä –§–ò–ù–ê–õ–¨–ù–´–ï –°–ß–ï–¢–ß–ò–ö–ò:")
        total_per_split = {}
        
        for split in ["train", "test", "valid"]:
            split_total = sum(validation_results["image_counts"][split].values())
            total_per_split[split] = split_total
            print(f"\n{split} ({split_total} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π):")
            
            for class_name, count in validation_results["image_counts"][split].items():
                percentage = (count / split_total) * 100 if split_total > 0 else 0
                print(f"   {class_name}: {count} ({percentage:.1f}%)")
        
        grand_total = sum(total_per_split.values())
        print(f"\nüéØ –ò–¢–û–ì–û: {grand_total} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        
        return validation_results

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
    
    integrator = NewDatasetIntegrator()
    
    print("üöÄ –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –ù–û–í–´–• –î–ê–¢–ê–°–ï–¢–û–í")
    print("="*80)
    
    # 1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º CSV –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
    print("\nüìä –®–ê–ì 1: –ê–ù–ê–õ–ò–ó CSV –ê–ù–ù–û–¢–ê–¶–ò–ô")
    
    all_analysis = {}
    for dataset_key in integrator.dataset_paths.keys():
        analysis = integrator.analyze_csv_annotations(dataset_key)
        all_analysis[dataset_key] = analysis
    
    # 2. –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    print("\nüîß –®–ê–ì 2: –°–û–ó–î–ê–ù–ò–ï –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–û–ì–û –î–ê–¢–ê–°–ï–¢–ê")
    
    output_path, integration_stats = integrator.create_integrated_dataset()
    
    # 3. –í–∞–ª–∏–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    print("\n‚úÖ –®–ê–ì 3: –í–ê–õ–ò–î–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–ê")
    
    validation_results = integrator.validate_integrated_dataset(output_path)
    
    # 4. –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    print(f"\nüéâ –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç: {output_path}")
    print(f"üñºÔ∏è –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {integration_stats['total_images']}")
    print(f"‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞: {'OK' if validation_results['structure_valid'] else '–û–®–ò–ë–ö–ê'}")
    
    return output_path, integration_stats, validation_results

if __name__ == "__main__":
    main()