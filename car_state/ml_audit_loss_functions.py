"""
ML Engineering Audit - –ê–Ω–∞–ª–∏–∑ Loss —Ñ—É–Ω–∫—Ü–∏–π –∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –æ–±—É—á–µ–Ω–∏—è
–î–µ—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ FocalLoss, class balancing –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from collections import Counter
from multiclass_damage_model import MulticlassDamageModel, FocalLoss

def analyze_loss_functions():
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ loss —Ñ—É–Ω–∫—Ü–∏–π –¥–ª—è imbalanced dataset"""
    
    print("üéØ ML ENGINEERING AUDIT - LOSS –§–£–ù–ö–¶–ò–ò –ò –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ê")
    print("="*65)
    
    # –†–µ–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –∏–∑ –Ω–∞—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    real_distribution = {
        0: 41,   # no_damage (6.3%)
        1: 278,  # minor_damage (42.8%)  
        2: 331   # major_damage (50.9%)
    }
    
    total_samples = sum(real_distribution.values())
    print(f"\n1Ô∏è‚É£ –ê–ù–ê–õ–ò–ó –†–ï–ê–õ–¨–ù–û–ì–û –î–ò–°–ë–ê–õ–ê–ù–°–ê –ö–õ–ê–°–°–û–í")
    print(f"   üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (–≤—Å–µ–≥–æ {total_samples}):")
    
    imbalance_ratios = {}
    for class_id, count in real_distribution.items():
        percentage = count / total_samples * 100
        class_name = ["no_damage", "minor_damage", "major_damage"][class_id]
        print(f"      –ö–ª–∞—Å—Å {class_id} ({class_name}): {count} ({percentage:.1f}%)")
        imbalance_ratios[class_id] = count / min(real_distribution.values())
    
    print(f"\n   ‚öñÔ∏è –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞):")
    for class_id, ratio in imbalance_ratios.items():
        class_name = ["no_damage", "minor_damage", "major_damage"][class_id]
        print(f"      {class_name}: {ratio:.2f}x")
    
    max_imbalance = max(imbalance_ratios.values())
    print(f"   üö® –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å: {max_imbalance:.2f}:1")
    
    if max_imbalance > 5:
        print(f"   ‚ùó –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –¥–∏—Å–±–∞–ª–∞–Ω—Å! –¢—Ä–µ–±—É–µ—Ç—Å—è —Å–µ—Ä—å–µ–∑–Ω–∞—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞")
    elif max_imbalance > 3:
        print(f"   ‚ö†Ô∏è –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å, —Ç—Ä–µ–±—É–µ—Ç—Å—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞")
    else:
        print(f"   ‚úÖ –£–º–µ—Ä–µ–Ω–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å")

def analyze_class_weights():
    """–ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤"""
    
    print(f"\n2Ô∏è‚É£ –ê–ù–ê–õ–ò–ó –°–¢–†–ê–¢–ï–ì–ò–ô –í–ï–°–û–í –ö–õ–ê–°–°–û–í")
    
    # –†–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    class_counts = torch.tensor([41, 278, 331], dtype=torch.float)
    total = class_counts.sum()
    
    print(f"   üìà –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤–µ—Å–æ–≤:")
    
    # 1. Inverse frequency (sklearn style)
    inv_freq_weights = total / (len(class_counts) * class_counts)
    print(f"   1. Inverse Frequency: {inv_freq_weights.numpy()}")
    print(f"      –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è: {(inv_freq_weights / inv_freq_weights.min()).numpy()}")
    
    # 2. Balanced (sklearn balanced)
    balanced_weights = total / (len(class_counts) * class_counts)
    print(f"   2. Balanced (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ): {balanced_weights.numpy()}")
    
    # 3. Square root balancing (–º–µ–Ω–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ)
    sqrt_weights = torch.sqrt(total / class_counts)
    sqrt_weights = sqrt_weights / sqrt_weights.min()
    print(f"   3. Square Root: {sqrt_weights.numpy()}")
    
    # 4. Log balancing (–µ—â–µ –º–µ–Ω–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ)
    log_weights = torch.log(total / class_counts + 1)
    log_weights = log_weights / log_weights.min() 
    print(f"   4. Log Balancing: {log_weights.numpy()}")
    
    # 5. Effective number of samples (–ø—Ä–∏ –±–æ–ª—å—à–∏—Ö –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞—Ö)
    beta = 0.9999
    effective_num = 1.0 - torch.pow(beta, class_counts)
    ens_weights = (1.0 - beta) / effective_num
    ens_weights = ens_weights / ens_weights.min()
    print(f"   5. Effective Number (Œ≤=0.9999): {ens_weights.numpy()}")
    
    print(f"\n   üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
    print(f"   - Inverse Frequency –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –Ω–∞—à–µ–≥–æ –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ 8:1")
    print(f"   - Square Root –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–∏—à–∫–æ–º –º—è–≥–∫–∏–º")
    print(f"   - Effective Number —Ö–æ—Ä–æ—à –¥–ª—è —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–≤")
    
    return inv_freq_weights

def test_focal_loss_variants():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ Focal Loss"""
    
    print(f"\n3Ô∏è‚É£ –£–ì–õ–£–ë–õ–ï–ù–ù–´–ô –ê–ù–ê–õ–ò–ó FOCAL LOSS")
    
    # –°–æ–∑–¥–∞–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    batch_size = 64
    num_classes = 3
    
    # –°–∏–º—É–ª–∏—Ä—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ logits (–¥–æ softmax)
    # –•–æ—Ä–æ—à–æ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –¥–∞–≤–∞—Ç—å —á–µ—Ç–∫–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    torch.manual_seed(42)
    confident_logits = torch.randn(batch_size, num_classes) * 2 + 1
    
    # –ü–ª–æ—Ö–æ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–∞–µ—Ç –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    uncertain_logits = torch.randn(batch_size, num_classes) * 0.5
    
    # –†–µ–∞–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏ —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º –∫–∞–∫ –≤ –¥–∞–Ω–Ω—ã—Ö
    real_targets = torch.cat([
        torch.zeros(4),  # 4/64 = 6.25% no_damage
        torch.ones(27),  # 27/64 = 42.2% minor_damage
        torch.full((33,), 2)  # 33/64 = 51.6% major_damage
    ]).long()
    
    # –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
    class_weights = torch.tensor([3.3333, 0.8333, 0.6667])
    
    print(f"   üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –¥–≤—É—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö:")
    print(f"      - Confident predictions (—Ö–æ—Ä–æ—à–æ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å)")
    print(f"      - Uncertain predictions (–ø–ª–æ—Ö–æ –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å)")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ gamma
    gammas = [0.0, 0.5, 1.0, 1.5, 2.0, 3.0, 5.0]
    alphas = [None, class_weights]
    
    print(f"\n   üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è CONFIDENT PREDICTIONS:")
    
    ce_baseline = F.cross_entropy(confident_logits, real_targets)
    print(f"      Baseline CrossEntropy: {ce_baseline:.4f}")
    
    for alpha_name, alpha in [("–±–µ–∑ –≤–µ—Å–æ–≤", None), ("—Å –≤–µ—Å–∞–º–∏", class_weights)]:
        print(f"\n      {alpha_name.upper()}:")
        for gamma in gammas:
            focal = FocalLoss(alpha=alpha, gamma=gamma)
            loss_val = focal(confident_logits, real_targets)
            reduction = loss_val / ce_baseline
            print(f"         Œ≥={gamma}: {loss_val:.4f} (√ó{reduction:.3f})")
    
    print(f"\n   üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è UNCERTAIN PREDICTIONS:")
    
    ce_baseline_unc = F.cross_entropy(uncertain_logits, real_targets)
    print(f"      Baseline CrossEntropy: {ce_baseline_unc:.4f}")
    
    for alpha_name, alpha in [("–±–µ–∑ –≤–µ—Å–æ–≤", None), ("—Å –≤–µ—Å–∞–º–∏", class_weights)]:
        print(f"\n      {alpha_name.upper()}:")
        for gamma in gammas:
            focal = FocalLoss(alpha=alpha, gamma=gamma)
            loss_val = focal(uncertain_logits, real_targets)
            reduction = loss_val / ce_baseline_unc
            print(f"         Œ≥={gamma}: {loss_val:.4f} (√ó{reduction:.3f})")
    
    print(f"\n   üí° –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è Focal Loss:")
    print(f"   - Œ≥=0: –æ–±—ã—á–Ω—ã–π CrossEntropy")
    print(f"   - Œ≥=1-2: —É–º–µ—Ä–µ–Ω–Ω–∞—è —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∞ –Ω–∞ hard examples")
    print(f"   - Œ≥=2-3: —Å–∏–ª—å–Ω–∞—è —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∞ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞)")
    print(f"   - Œ≥>3: –æ—á–µ–Ω—å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∞ (—Ä–∏—Å–∫ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏)")

def analyze_training_dynamics():
    """–ê–Ω–∞–ª–∏–∑ –¥–∏–Ω–∞–º–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""
    
    print(f"\n4Ô∏è‚É£ –ê–ù–ê–õ–ò–ó –î–ò–ù–ê–ú–ò–ö–ò –û–ë–£–ß–ï–ù–ò–Ø")
    
    # –°–∏–º—É–ª–∏—Ä—É–µ–º –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è
    model = MulticlassDamageModel(num_classes=3)
    class_weights = torch.tensor([3.3333, 0.8333, 0.6667])
    
    # –†–∞–∑–Ω—ã–µ loss —Ñ—É–Ω–∫—Ü–∏–∏
    ce_loss = nn.CrossEntropyLoss(weight=class_weights)
    focal_loss = FocalLoss(alpha=class_weights, gamma=2.0)
    
    # –°–∏–º—É–ª–∏—Ä—É–µ–º —ç–ø–æ—Ö–∏ –æ–±—É—á–µ–Ω–∏—è
    epochs_data = []
    torch.manual_seed(42)
    
    for epoch in range(10):
        # –í –Ω–∞—á–∞–ª–µ –æ–±—É—á–µ–Ω–∏—è - –Ω–µ—É–≤–µ—Ä–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        # –ü–æ –º–µ—Ä–µ –æ–±—É—á–µ–Ω–∏—è - –±–æ–ª–µ–µ —É–≤–µ—Ä–µ–Ω–Ω—ã–µ
        confidence_progress = epoch / 10.0
        noise_level = 2.0 * (1 - confidence_progress) + 0.5 * confidence_progress
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º logits –¥–ª—è —ç–ø–æ—Ö–∏
        batch_logits = torch.randn(32, 3) * noise_level
        batch_targets = torch.randint(0, 3, (32,))
        
        # –°—á–∏—Ç–∞–µ–º loss
        ce_val = ce_loss(batch_logits, batch_targets).item()
        focal_val = focal_loss(batch_logits, batch_targets).item()
        
        epochs_data.append({
            'epoch': epoch,
            'ce_loss': ce_val,
            'focal_loss': focal_val,
            'focal_reduction': focal_val / ce_val
        })
    
    print(f"   üìà –°–∏–º—É–ª—è—Ü–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è:")
    print(f"      –≠–ø–æ—Ö–∞ | CrossEntropy | Focal Loss | Reduction")
    print(f"      ------|-------------|------------|----------")
    
    for data in epochs_data:
        print(f"      {data['epoch']:3d}   | {data['ce_loss']:8.4f}    | {data['focal_loss']:7.4f}   | √ó{data['focal_reduction']:.3f}")
    
    avg_reduction = np.mean([d['focal_reduction'] for d in epochs_data])
    print(f"\n   üìä –°—Ä–µ–¥–Ω—è—è —Ä–µ–¥—É–∫—Ü–∏—è Focal Loss: √ó{avg_reduction:.3f}")
    
    if avg_reduction < 0.8:
        print(f"   ‚úÖ Focal Loss —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ easy examples")
    else:
        print(f"   ‚ö†Ô∏è Focal Loss —Å–ª–∞–±–æ –≤–ª–∏—è–µ—Ç, –≤–æ–∑–º–æ–∂–Ω–æ —Å—Ç–æ–∏—Ç —É–≤–µ–ª–∏—á–∏—Ç—å Œ≥")

def analyze_optimizer_settings():
    """–ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è CPU –æ–±—É—á–µ–Ω–∏—è"""
    
    print(f"\n5Ô∏è‚É£ –ê–ù–ê–õ–ò–ó –ù–ê–°–¢–†–û–ï–ö –û–ü–¢–ò–ú–ò–ó–ê–¢–û–†–ê")
    
    model = MulticlassDamageModel(num_classes=3)
    
    print(f"   üîß –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è CPU –æ–±—É—á–µ–Ω–∏—è:")
    
    # Learning rates –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    print(f"\n   üìö Learning Rate —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏:")
    
    backbone_params = []
    classifier_params = []
    
    for name, param in model.named_parameters():
        if 'backbone' in name:
            backbone_params.append(param)
        else:
            classifier_params.append(param)
    
    # –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ learning rates
    base_lr = 1e-4  # –î–ª—è CPU
    
    print(f"      Base LR (CPU): {base_lr}")
    print(f"      Backbone LR: {base_lr * 0.1:.1e} (√ó0.1 - pretrained weights)")
    print(f"      Classifier LR: {base_lr * 1.0:.1e} (√ó1.0 - –Ω–æ–≤—ã–µ —Å–ª–æ–∏)")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
    print(f"\n   ‚öôÔ∏è –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä AdamW:")
    print(f"      Learning Rate: {base_lr:.1e}")
    print(f"      Weight Decay: 1e-4 (L2 regularization)")
    print(f"      Betas: (0.9, 0.999) (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –¥–ª—è Adam)")
    print(f"      Eps: 1e-8")
    
    # Scheduler
    print(f"\n   üìà Learning Rate Scheduler:")
    print(f"      –¢–∏–ø: ReduceLROnPlateau")
    print(f"      –ú–µ—Ç—Ä–∏–∫–∞: macro F1-score (–≤–∞–∂–Ω–æ –¥–ª—è imbalanced data)")
    print(f"      Factor: 0.5 (—Å–Ω–∏–∂–µ–Ω–∏–µ –≤ 2 —Ä–∞–∑–∞)")
    print(f"      Patience: 3 —ç–ø–æ—Ö–∏")
    print(f"      Min LR: 1e-7")
    
    # Gradient clipping
    print(f"\n   ‚úÇÔ∏è Gradient Clipping:")
    print(f"      Max norm: 1.0 (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç gradient explosion)")
    print(f"      –û—Å–æ–±–µ–Ω–Ω–æ –≤–∞–∂–Ω–æ —Å Focal Loss –∏ class weights")
    
    print(f"\n   üí° –ü–æ—á–µ–º—É —ç—Ç–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:")
    print(f"   - –ù–∏–∑–∫–∏–π LR –¥–ª—è CPU (–º–µ–¥–ª–µ–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è)")
    print(f"   - Differential LR (backbone –∑–Ω–∞–µ—Ç features, classifier —É—á–∏—Ç—Å—è)")
    print(f"   - F1-metric –¥–ª—è scheduler (–≤–∞–∂–Ω–æ –¥–ª—è imbalanced classes)")
    print(f"   - Weight decay –ø—Ä–æ—Ç–∏–≤ overfitting –Ω–∞ –º–∞–ª–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ")

def analyze_memory_and_performance():
    """–ê–Ω–∞–ª–∏–∑ –ø–∞–º—è—Ç–∏ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    
    print(f"\n6Ô∏è‚É£ –ê–ù–ê–õ–ò–ó –ü–ê–ú–Ø–¢–ò –ò –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò")
    
    model = MulticlassDamageModel(num_classes=3)
    
    # –ü–æ–¥—Å—á–µ—Ç –ø–∞–º—è—Ç–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö batch sizes
    batch_sizes = [1, 4, 8, 16, 32]
    
    print(f"   üíæ –ü–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏):")
    print(f"      Batch | Model Mem | Forward | Backward | Total")
    print(f"      ------|-----------|---------|----------|--------")
    
    model_size_mb = sum(p.numel() * 4 for p in model.parameters()) / (1024**2)
    
    for batch_size in batch_sizes:
        # –ü—Ä–∏–º–µ—Ä–Ω—ã–µ —Ä–∞—Å—á–µ—Ç—ã –¥–ª—è ResNet50
        input_size_mb = batch_size * 3 * 224 * 224 * 4 / (1024**2)
        forward_mem_mb = input_size_mb * 4  # –ê–∫—Ç–∏–≤–∞—Ü–∏–∏
        backward_mem_mb = forward_mem_mb * 2  # –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã
        total_mb = model_size_mb + forward_mem_mb + backward_mem_mb
        
        print(f"      {batch_size:3d}   | {model_size_mb:6.1f} MB | {forward_mem_mb:5.1f} MB | {backward_mem_mb:6.1f} MB | {total_mb:6.1f} MB")
    
    print(f"\n   ‚è±Ô∏è –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (–æ—Ü–µ–Ω–∫–∏ –¥–ª—è CPU):")
    
    total_samples = 650
    validation_split = 0.2
    train_samples = int(total_samples * (1 - validation_split))
    
    for batch_size in [8, 16, 32]:
        batches_per_epoch = train_samples // batch_size
        seconds_per_batch = 2.0 if batch_size <= 16 else 3.0  # CPU –º–µ–¥–ª–µ–Ω–Ω–µ–µ
        minutes_per_epoch = (batches_per_epoch * seconds_per_batch) / 60
        
        print(f"      Batch {batch_size}: {batches_per_epoch} batches/epoch, ~{minutes_per_epoch:.1f} min/epoch")
    
    recommended_batch = 16
    recommended_epochs = 20
    total_hours = (train_samples // recommended_batch) * 2.0 * recommended_epochs / 3600
    
    print(f"\n   üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è:")
    print(f"      –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π batch size: {recommended_batch}")
    print(f"      –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö: {recommended_epochs}")
    print(f"      –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: ~{total_hours:.1f} —á–∞—Å–æ–≤")
    
    print(f"\n   ‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è CPU:")
    print(f"   - Batch size > 32 –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω—ã–º")
    print(f"   - num_workers = 2 (–Ω–µ –±–æ–ª—å—à–µ, CPU –æ–≥—Ä–∞–Ω–∏—á–µ–Ω)")
    print(f"   - pin_memory = False (–Ω–µ –Ω—É–∂–Ω–æ –±–µ–∑ GPU)")
    print(f"   - persistent_workers = False (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ loss —Ñ—É–Ω–∫—Ü–∏–π"""
    
    analyze_loss_functions()
    class_weights = analyze_class_weights()
    test_focal_loss_variants()
    analyze_training_dynamics()
    analyze_optimizer_settings()
    analyze_memory_and_performance()
    
    print(f"\nüéØ –ò–¢–û–ì–û–í–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û LOSS –ò –û–ë–£–ß–ï–ù–ò–Æ:")
    print("="*65)
    print("‚úÖ FocalLoss —Å Œ≥=2.0 –æ–ø—Ç–∏–º–∞–ª–µ–Ω –¥–ª—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ 8:1")
    print("‚úÖ Class weights = [3.33, 0.83, 0.67] –∫–æ–º–ø–µ–Ω—Å–∏—Ä—É—é—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å")
    print("‚úÖ AdamW —Å LR=1e-4 –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è CPU –æ–±—É—á–µ–Ω–∏—è")
    print("‚úÖ ReduceLROnPlateau –ø–æ F1-score –¥–ª—è imbalanced data")
    print("‚úÖ Batch size = 16 –æ–ø—Ç–∏–º–∞–ª–µ–Ω –¥–ª—è CPU –∏ –ø–∞–º—è—Ç–∏")
    print("‚úÖ Weight decay = 1e-4 –ø—Ä–æ—Ç–∏–≤ overfitting")
    print("‚úÖ Gradient clipping = 1.0 –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏")
    
    print(f"\n‚ö†Ô∏è –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ú–û–ú–ï–ù–¢–´:")
    print("‚ùó –î–∏—Å–±–∞–ª–∞–Ω—Å 8:1 —Ç—Ä–µ–±—É–µ—Ç –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û–ô –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏")
    print("‚ùó –ú–∞–ª—ã–π –∫–ª–∞—Å—Å no_damage (6.3%) - —Ä–∏—Å–∫ poor recall")
    print("‚ùó CPU –æ–±—É—á–µ–Ω–∏–µ –∑–∞–π–º–µ—Ç ~3-4 —á–∞—Å–∞ –¥–ª—è 20 —ç–ø–æ—Ö")
    print("‚ùó –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –º–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å F1-score –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞")
    print("‚ùó Early stopping –ø–æ macro F1, –Ω–µ –ø–æ loss!")
    
    print(f"\nüìà –°–¢–†–ê–¢–ï–ì–ò–Ø –û–ë–£–ß–ï–ù–ò–Ø:")
    print("1. –ù–∞—á–∞—Ç—å —Å 5 —ç–ø–æ—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏")
    print("2. –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å F1-score –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ –æ—Ç–¥–µ–ª—å–Ω–æ")
    print("3. –ï—Å–ª–∏ no_damage F1 < 0.5, —É–≤–µ–ª–∏—á–∏—Ç—å –µ–≥–æ –≤–µ—Å –≤ 2 —Ä–∞–∑–∞")
    print("4. –ï—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ, –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –¥–æ 20-30 —ç–ø–æ—Ö")
    print("5. –°–æ—Ö—Ä–∞–Ω—è—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ macro F1-score")

if __name__ == "__main__":
    main()