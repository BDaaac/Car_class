"""
–°–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –¥–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å —Å 3 –∫–ª–∞—Å—Å–∞–º–∏
"""
import os
import re
import json
import random
import math
from pathlib import Path
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from torchvision import transforms
from PIL import Image
from collections import Counter
from sklearn.metrics import f1_score, roc_curve, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import sys

# –ò–º–ø–æ—Ä—Ç –Ω–∞—à–∏—Ö –º–æ–¥—É–ª–µ–π —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø—É—Ç—è–º–∏
try:
    from .multiclass_damage_model import MulticlassDamageModel, FocalLoss, create_training_transforms, create_validation_transforms
except ImportError:
    # Fallback –¥–ª—è –ø—Ä—è–º–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
    current_dir = os.path.dirname(os.path.abspath(__file__))
    if current_dir not in sys.path:
        sys.path.append(current_dir)
    from multiclass_damage_model import MulticlassDamageModel, FocalLoss, create_training_transforms, create_validation_transforms

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
RANDOM_SEED = 42

# –ü—É—Ç–∏ –∫ –¥–∞—Ç–∞—Å–µ—Ç–∞–º (–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞)
DATASET_ROOTS = [
    r"C:\Users\–î–∏–º–∞—à\Desktop\python\hackaton\data\Rust and Scrach.v1i.multiclass\train",
    r"C:\Users\–î–∏–º–∞—à\Desktop\python\hackaton\data\Car Scratch and Dent.v5i.multiclass\train",
    r"C:\Users\–î–∏–º–∞—à\Desktop\python\hackaton\data\Dent_Detection.v1i.multiclass\train",
    r"C:\Users\–î–∏–º–∞—à\Desktop\python\hackaton\data\integrated_multiclass_dataset",  # –ù–æ–≤—ã–π –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
]

def convert_multilabel_to_multiclass(row, dataset_type):
    """
    –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç Multi-Label –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –≤ Multi-Class
    
    –õ–æ–≥–∏–∫–∞ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏:
    - major_damage (2): —Å–µ—Ä—å–µ–∑–Ω—ã–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è (—Ä–∂–∞–≤—á–∏–Ω–∞, –≤–º—è—Ç–∏–Ω—ã)
    - minor_damage (1): –ª–µ–≥–∫–∏–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è (—Ü–∞—Ä–∞–ø–∏–Ω—ã)
    - no_damage (0): –Ω–µ—Ç –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π
    """
    if dataset_type == "car_scratch_dent":
        # Car Scratch and Dent dataset: dent, dirt, scratch
        if row['dent'] == 1:
            return 2  # major_damage (–≤–º—è—Ç–∏–Ω—ã - —Å–µ—Ä—å–µ–∑–Ω–æ)
        elif row['scratch'] == 1:
            return 1  # minor_damage (—Ü–∞—Ä–∞–ø–∏–Ω—ã - –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ)
        else:
            return 0  # no_damage (—Ö–æ—Ç—è —Ç–∞–∫–∏—Ö –ø–æ—á—Ç–∏ –Ω–µ—Ç –≤ —ç—Ç–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ)
            
    elif dataset_type == "rust_scratch":
        # Rust and Scratch dataset: car, dunt, rust, scracth
        if row['rust'] == 1 or row['dunt'] == 1:
            return 2  # major_damage (—Ä–∂–∞–≤—á–∏–Ω–∞/–≤–º—è—Ç–∏–Ω—ã - —Å–µ—Ä—å–µ–∑–Ω–æ)
        elif row['scracth'] == 1:  # –û–ø–µ—á–∞—Ç–∫–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ —Å—Ç–æ–ª–±—Ü–∞
            return 1  # minor_damage (—Ü–∞—Ä–∞–ø–∏–Ω—ã - –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ)
        elif row['car'] == 1:
            return 0  # no_damage (—á–∏—Å—Ç–∞—è –º–∞—à–∏–Ω–∞)
        else:
            return 1  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é minor –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –Ω–µ–ø–æ–Ω—è—Ç–Ω–æ
    
    elif dataset_type == "dent_detection":
        # Dent Detection dataset: –±–∏–Ω–∞—Ä–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞ "dent" (0/1)
        if row['dent'] == 1:
            return 1  # minor_damage (–≤–º—è—Ç–∏–Ω—ã –∫–∞–∫ –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ)
        else:
            return 0  # no_damage (–Ω–µ—Ç –≤–º—è—Ç–∏–Ω)
    
    return 1  # fallback

def normalize_columns(df):
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ CSV —Ñ–∞–π–ª–æ–≤ –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏
    """
    # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    df = df.rename(columns={c: c.strip().lower() for c in df.columns})
    
    # –£–Ω–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ –∏–º–µ–Ω–∞ (–∏—Å–ø—Ä–∞–≤–ª—è–µ–º –æ–ø–µ—á–∞—Ç–∫–∏)
    rename_map = {
        "scracth": "scratch",  # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –æ–ø–µ—á–∞—Ç–∫—É –≤ Rust dataset
        "dunt": "dent",        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –æ–ø–µ—á–∞—Ç–∫—É –≤ Rust dataset
        " dent": "dent",       # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π
    }
    df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})
    
    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –±–∏–Ω–∞—Ä–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã –Ω—É–ª—è–º–∏
    for col in ["dent", "scratch", "rust", "clean", "car", "dirt"]:
        if col not in df.columns:
            df[col] = 0
    
    # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞
    if "image_path" not in df.columns and "filename" in df.columns:
        df = df.rename(columns={"filename": "image_path"})
    
    return df

def collect_images_from_roots(roots):
    """
    –°–æ–±–∏—Ä–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ CSV —Ñ–∞–π–ª–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏ –Ω–æ–≤–æ–π –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    
    Returns:
        tuple: (items, class_distribution)
            items: —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–ø—É—Ç—å_–∫_—Ñ–∞–π–ª—É, –∫–ª–∞—Å—Å_id)
            class_distribution: Counter —Å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –∫–ª–∞—Å—Å–æ–≤
    """
    items = []
    class_distribution = Counter()
    
    print("üîç –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –ò–ó –î–ê–¢–ê–°–ï–¢–û–í")
    
    for root in roots:
        root = Path(root)
        if not root.exists():
            print(f"‚ùå –ü—É—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω: {root}")
            continue
            
        print(f"\nüìÅ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {root}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —ç—Ç–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–ª–∏ CSV-based
        if "integrated_multiclass_dataset" in str(root):
            print("   üÜï –û–±–Ω–∞—Ä—É–∂–µ–Ω –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å train/test/valid —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π")
            integrated_items, integrated_dist = load_integrated_dataset(root)
            items.extend(integrated_items)
            class_distribution.update(integrated_dist)
        else:
            # –û–±—ã—á–Ω–∞—è CSV –ª–æ–≥–∏–∫–∞
            csv_files = list(root.glob("*.csv"))
            if not csv_files:
                print(f"   ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω CSV —Ñ–∞–π–ª –≤ {root}")
                continue
            
            csv_file = csv_files[0]
            print(f"   üìÑ –ù–∞–π–¥–µ–Ω CSV: {csv_file.name}")
            
            csv_items, csv_dist = load_csv_dataset(root, csv_file)
            items.extend(csv_items)
            class_distribution.update(csv_dist)
    
    print(f"\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(items)}")
    
    total = sum(class_distribution.values())
    for class_id in [0, 1, 2]:
        class_name = ["no_damage", "minor_damage", "major_damage"][class_id]
        count = class_distribution[class_id]
        percentage = (count / total * 100) if total > 0 else 0
        print(f"   {class_id} ({class_name}): {count} ({percentage:.1f}%)")
    
    return items, class_distribution

def load_integrated_dataset(root_path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å train/test/valid —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
    """
    items = []
    class_distribution = Counter()
    
    class_mapping = {
        "no_damage": 0,
        "minor_damage": 1, 
        "major_damage": 2
    }
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ split'—ã (train, test, valid)
    for split_name in ["train", "test", "valid"]:
        split_path = root_path / split_name
        if not split_path.exists():
            continue
            
        print(f"   üìÇ Split: {split_name}")
        
        for class_name, class_id in class_mapping.items():
            class_path = split_path / class_name
            if not class_path.exists():
                continue
                
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –ø–∞–ø–∫–∏ –∫–ª–∞—Å—Å–∞
            image_files = []
            for ext in ["*.jpg", "*.jpeg", "*.png", "*.bmp"]:
                image_files.extend(class_path.glob(ext))
            
            for img_path in image_files:
                items.append((str(img_path), class_id))
                class_distribution[class_id] += 1
            
            print(f"      {class_name}: {len(image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    return items, class_distribution

def load_csv_dataset(root_path, csv_file):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ CSV-based –¥–∞—Ç–∞—Å–µ—Ç–∞
    """
    items = []
    class_distribution = Counter()
    
    try:
        df = pd.read_csv(csv_file)
        print(f"   üìä –°—Ç—Ä–æ–∫ –≤ CSV: {len(df)}")
        print(f"   üìã –ò—Å—Ö–æ–¥–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
        df = normalize_columns(df)
        print(f"   üìã –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è CSV: {e}")
        return items, class_distribution
            
        csv_file = csv_files[0]
        print(f"   üìÑ –ù–∞–π–¥–µ–Ω CSV: {csv_file.name}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º CSV
        try:
            df = pd.read_csv(csv_file)
            print(f"   üìä –°—Ç—Ä–æ–∫ –≤ CSV: {len(df)}")
            print(f"   üìã –ò—Å—Ö–æ–¥–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
            df = normalize_columns(df)
            print(f"   üìã –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è CSV: {e}")
            return items, class_distribution
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ —Å—Ç–æ–ª–±—Ü–∞–º
        columns = set(df.columns)
        if {'image_path', 'dent', 'dirt', 'scratch'}.issubset(columns):
            dataset_type = "car_scratch_dent"
            print(f"   üè∑Ô∏è –¢–∏–ø –¥–∞—Ç–∞—Å–µ—Ç–∞: Car Scratch and Dent")
        elif {'image_path', 'car', 'dent', 'rust', 'scratch'}.issubset(columns):
            dataset_type = "rust_scratch"  
            print(f"   üè∑Ô∏è –¢–∏–ø –¥–∞—Ç–∞—Å–µ—Ç–∞: Rust and Scratch")
        elif {'image_path', 'dent'}.issubset(columns) and len(columns) <= 3:
            dataset_type = "dent_detection"
            print(f"   üè∑Ô∏è –¢–∏–ø –¥–∞—Ç–∞—Å–µ—Ç–∞: Dent Detection (binary)")
        else:
            print(f"   ‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç CSV. –ö–æ–ª–æ–Ω–∫–∏: {columns}")
            print(f"   ‚ùå –û–∂–∏–¥–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã:")
            print(f"      - Car Scratch: ['image_path', 'dent', 'dirt', 'scratch']")
            print(f"      - Rust Scratch: ['image_path', 'car', 'dent', 'rust', 'scratch']") 
            print(f"      - Dent Detection: ['image_path', 'dent']")
            return items, class_distribution
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–æ–∫—É
        processed_count = 0
        debug_mapping = {"dent_detection": {"minor": 0, "no_damage": 0}}
        
        for idx, row in df.iterrows():
            image_name = row['image_path']
            image_path = root / image_name
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
            if not image_path.exists():
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–π–ª—ã –º–æ–ª—á–∞
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ multi-class
            class_id = convert_multilabel_to_multiclass(row, dataset_type)
            
            # –û—Ç–ª–∞–¥–∫–∞ –¥–ª—è Dent Detection
            if dataset_type == "dent_detection":
                dent_val = row.get('dent', 0)
                if dent_val == 1:
                    debug_mapping["dent_detection"]["minor"] += 1
                else:
                    debug_mapping["dent_detection"]["no_damage"] += 1
                if processed_count < 5:  # –ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫
                    print(f"      DEBUG: row {idx}: dent={dent_val} ‚Üí class_id={class_id}")
            
            items.append((str(image_path), class_id))
            class_distribution[class_id] += 1
            processed_count += 1
        
        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è Dent Detection
        if dataset_type == "dent_detection":
            print(f"   üîç –û–¢–õ–ê–î–ö–ê Dent Detection:")
            print(f"      dent=1 ‚Üí minor_damage: {debug_mapping['dent_detection']['minor']}")
            print(f"      dent=0 ‚Üí no_damage: {debug_mapping['dent_detection']['no_damage']}")
        
        print(f"   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {processed_count}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —ç—Ç–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É
        local_dist = Counter()
        for _, class_id in items[-processed_count:]:
            local_dist[class_id] += 1
        
        print(f"   üìà –õ–æ–∫–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
        for class_id in [0, 1, 2]:
            class_name = ["no_damage", "minor_damage", "major_damage"][class_id]
            count = local_dist[class_id]
            print(f"      {class_id} ({class_name}): {count}")
    
    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    random.Random(RANDOM_SEED).shuffle(items)
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(items)}")
    total_images = len(items)
    for class_id in [0, 1, 2]:
        class_name = ["no_damage", "minor_damage", "major_damage"][class_id]
        count = class_distribution[class_id]
        percentage = (count / total_images * 100) if total_images > 0 else 0
        print(f"   {class_id} ({class_name}): {count} ({percentage:.1f}%)")
    
    return items, class_distribution

class MulticlassDamageDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π"""
    
    def __init__(self, items, transforms=None):
        self.items = items
        self.transforms = transforms
        
    def __len__(self):
        return len(self.items)
    
    def __getitem__(self, idx):
        img_path, class_id = self.items[idx]
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = Image.open(img_path).convert('RGB')
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
            if self.transforms:
                image = self.transforms(image)
            
            return image, class_id
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {img_path}: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —á–µ—Ä–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞–∫ fallback
            if self.transforms:
                dummy = self.transforms(Image.new('RGB', (224, 224), (0, 0, 0)))
            else:
                dummy = torch.zeros(3, 224, 224)
            return dummy, class_id

def split_train_validation(items, val_ratio=0.3, seed=RANDOM_SEED):
    """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–π –∫–ª–∞—Å—Å–æ–≤ (—É–≤–µ–ª–∏—á–µ–Ω–∞ –¥–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏)"""
    random.Random(seed).shuffle(items)
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–ª–∞—Å—Å–∞–º
    class_items = {0: [], 1: [], 2: []}
    for item in items:
        class_items[item[1]].append(item)
    
    train_items = []
    val_items = []
    
    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ –¥–µ–ª–∞–µ–º split
    for class_id, class_data in class_items.items():
        n_val = int(len(class_data) * val_ratio)
        val_items.extend(class_data[:n_val])
        train_items.extend(class_data[n_val:])
    
    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º
    random.Random(seed).shuffle(train_items)
    random.Random(seed).shuffle(val_items)
    
    print(f"üìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (validation {val_ratio*100:.0f}%):")
    print(f"   Train: {len(train_items)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    print(f"   Validation: {len(val_items)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º
    train_dist = Counter([item[1] for item in train_items])
    val_dist = Counter([item[1] for item in val_items])
    
    print(f"   üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ train:")
    for class_id in [0, 1, 2]:
        class_name = ["no_damage", "minor_damage", "major_damage"][class_id]
        count = train_dist[class_id]
        print(f"      {class_id} ({class_name}): {count}")
    
    print(f"   üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ validation:")
    for class_id in [0, 1, 2]:
        class_name = ["no_damage", "minor_damage", "major_damage"][class_id]
        count = val_dist[class_id]
        print(f"      {class_id} ({class_name}): {count}")
    
    return train_items, val_items

def load_integrated_dataset_split(root_path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å —É–∂–µ –≥–æ—Ç–æ–≤—ã–º split'–æ–º train/test/valid
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∑–∞–ø–∏—Å–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    """
    train_records = []
    val_records = []
    
    class_mapping = {
        "no_damage": 0,
        "minor_damage": 1, 
        "major_damage": 2
    }
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º train split
    train_path = root_path / "train"
    if train_path.exists():
        print("   üìÇ –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è train split")
        for class_name, class_id in class_mapping.items():
            class_path = train_path / class_name
            if not class_path.exists():
                continue
                
            image_files = []
            for ext in ["*.jpg", "*.jpeg", "*.png", "*.bmp"]:
                image_files.extend(class_path.glob(ext))
            
            for img_path in image_files:
                record = {
                    'path': str(img_path),
                    'label': int(class_id),
                    'source': 'integrated_dataset',
                    'dataset_type': 'integrated'
                }
                train_records.append(record)
            
            print(f"      {class_name}: {len(image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º valid split (–∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –≤–∞–ª–∏–¥–∞—Ü–∏—é)
    valid_path = root_path / "valid"
    if valid_path.exists():
        print("   üìÇ –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è valid split")
        for class_name, class_id in class_mapping.items():
            class_path = valid_path / class_name
            if not class_path.exists():
                continue
                
            image_files = []
            for ext in ["*.jpg", "*.jpeg", "*.png", "*.bmp"]:
                image_files.extend(class_path.glob(ext))
            
            for img_path in image_files:
                record = {
                    'path': str(img_path),
                    'label': int(class_id),
                    'source': 'integrated_dataset',
                    'dataset_type': 'integrated'
                }
                val_records.append(record)
            
            print(f"      {class_name}: {len(image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    print(f"   ‚úÖ Train: {len(train_records)}, Valid: {len(val_records)}")
    return train_records, val_records

def proper_dataset_split(roots, val_ratio=0.3, seed=42):
    """
    –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–∞—Ç–∞—Å–µ—Ç–∞–º:
    - Dent_Detection ‚Üí —Ç–æ–ª—å–∫–æ train
    - –û—Å—Ç–∞–ª—å–Ω—ã–µ ‚Üí stratified train/val split
    - –°–æ—Ö—Ä–∞–Ω—è–µ–º source –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å—ç–º–ø–ª–∞
    """
    from sklearn.model_selection import train_test_split
    import numpy as np
    
    print("üîÑ –ü–†–ê–í–ò–õ–¨–ù–û–ï –†–ê–ó–î–ï–õ–ï–ù–ò–ï –ü–û –î–ê–¢–ê–°–ï–¢–ê–ú")
    print("   Dent_Detection ‚Üí –¢–û–õ–¨–ö–û train")
    print("   –û—Å—Ç–∞–ª—å–Ω—ã–µ ‚Üí —á–µ—Å—Ç–Ω—ã–π stratified split")
    
    train_records = []
    val_records = []
    
    for root in roots:
        root_path = Path(root)
        
        print(f"\nüìÅ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {root_path.name}")
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        if "integrated_multiclass_dataset" in str(root_path):
            print("   üÜï –û–±–Ω–∞—Ä—É–∂–µ–Ω –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç")
            integrated_train, integrated_val = load_integrated_dataset_split(root_path)
            train_records.extend(integrated_train)
            val_records.extend(integrated_val)
            continue
        
        # –ò—â–µ–º CSV —Ñ–∞–π–ª –¥–ª—è –æ–±—ã—á–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        csv_files = list(root_path.glob("*.csv"))
        if not csv_files:
            print(f"   ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω CSV —Ñ–∞–π–ª –≤ {root_path}")
            continue
            
        csv_file = csv_files[0]
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º CSV
        try:
            df = pd.read_csv(csv_file)
            df = normalize_columns(df)
            
            print(f"   üìÑ CSV —Ñ–∞–π–ª: {csv_file.name}")
            print(f"   üìä –°—Ç—Ä–æ–∫ –≤ CSV: {len(df)}")
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è CSV: {e}")
            continue
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–º—è –¥–∞—Ç–∞—Å–µ—Ç–∞ (—Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∞—è –ø–∞–ø–∫–∞, –∞ –Ω–µ "train")
        dataset_name = root_path.parent.name.lower()  # –ü–æ–ª—É—á–∞–µ–º –∏–º—è –¥–∞—Ç–∞—Å–µ—Ç–∞ 
        source_name = dataset_name  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–º—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        
        print(f"   üè∑Ô∏è –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö: {dataset_name}")
        
        columns = set(df.columns)
        
        if {'image_path', 'dent', 'dirt', 'scratch'}.issubset(columns):
            dataset_type = "car_scratch_dent"
            print(f"   üè∑Ô∏è –¢–∏–ø: Car Scratch and Dent")
        elif {'image_path', 'car', 'dent', 'rust', 'scratch'}.issubset(columns):
            dataset_type = "rust_scratch"  
            print(f"   üè∑Ô∏è –¢–∏–ø: Rust and Scratch")
        elif {'image_path', 'dent'}.issubset(columns) and len(columns) <= 3:
            dataset_type = "dent_detection"
            print(f"   üè∑Ô∏è –¢–∏–ø: Dent Detection (binary)")
        else:
            print(f"   ‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç CSV")
            continue
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å–∏ —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        records = []
        processed_count = 0
        
        for idx, row in df.iterrows():
            image_name = row['image_path']
            image_path = root_path / image_name
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
            if not image_path.exists():
                continue
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ multi-class
            class_id = convert_multilabel_to_multiclass(row, dataset_type)
            
            # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—É—é –∑–∞–ø–∏—Å—å
            record = {
                'path': str(image_path),
                'label': int(class_id),
                'source': source_name,
                'dataset_type': dataset_type
            }
            
            records.append(record)
            processed_count += 1
        
        print(f"   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {processed_count}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ª–æ–∫–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        local_dist = Counter([rec['label'] for rec in records])
        print(f"   üìà –õ–æ–∫–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
        for class_id in [0, 1, 2]:
            class_name = ["no_damage", "minor_damage", "major_damage"][class_id]
            count = local_dist[class_id]
            percentage = (count / processed_count * 100) if processed_count > 0 else 0
            print(f"      {class_id} ({class_name}): {count} ({percentage:.1f}%)")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ü–†–ê–í–ò–õ–¨–ù–û–ì–û –∏–º–µ–Ω–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
        if "dent_detection" in dataset_name:
            # ‚ùó Dent_Detection –ø–æ–ª–Ω–æ—Å—Ç—å—é –≤ train
            train_records.extend(records)
            print(f"   üü° –î–æ–±–∞–≤–ª–µ–Ω–æ –≤ TRAIN (—Ç–æ–ª—å–∫–æ): {len(records)} –∑–∞–ø–∏—Å–µ–π")
        else:
            # –ß–µ—Å—Ç–Ω—ã–π stratified split –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
            if len(records) > 0:
                labels = np.array([rec['label'] for rec in records])
                indices = np.arange(len(records))
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è stratify
                unique_labels = np.unique(labels)
                if len(unique_labels) > 1:
                    try:
                        train_idx, val_idx = train_test_split(
                            indices,
                            test_size=val_ratio,
                            random_state=seed,
                            stratify=labels
                        )
                    except ValueError as e:
                        print(f"   ‚ö†Ô∏è Stratify failed: {e}, using random split")
                        # –ï—Å–ª–∏ stratify –Ω–µ —É–¥–∞–µ—Ç—Å—è, –¥–µ–ª–∞–µ–º –æ–±—ã—á–Ω—ã–π split
                        train_idx, val_idx = train_test_split(
                            indices,
                            test_size=val_ratio,
                            random_state=seed
                        )
                else:
                    # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å, –¥–µ–ª–∞–µ–º –ø—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
                    split_point = int(len(records) * (1 - val_ratio))
                    train_idx = indices[:split_point]
                    val_idx = indices[split_point:]
                
                train_subset = [records[i] for i in train_idx]
                val_subset = [records[i] for i in val_idx]
                
                train_records.extend(train_subset)
                val_records.extend(val_subset)
                
                print(f"   üü¢ –î–æ–±–∞–≤–ª–µ–Ω–æ –≤ TRAIN: {len(train_subset)} –∑–∞–ø–∏—Å–µ–π")
                print(f"   üü¢ –î–æ–±–∞–≤–ª–µ–Ω–æ –≤ VAL: {len(val_subset)} –∑–∞–ø–∏—Å–µ–π")
    
    # –ñ—ë—Å—Ç–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ Dent_Detection –Ω–µ –ø–æ–ø–∞–ª –≤ validation
    dent_in_val = [rec for rec in val_records if "dent_detection" in rec['source']]
    if len(dent_in_val) > 0:
        print(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: Dent_Detection –ø–æ–ø–∞–ª –≤ validation: {len(dent_in_val)} –∑–∞–ø–∏—Å–µ–π!")
        print(f"   –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ val: {set(rec['source'] for rec in dent_in_val)}")
        raise AssertionError("Dent_Detection –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¢–û–õ–¨–ö–û –≤ train!")
    
    print(f"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–π–¥–µ–Ω–∞: Dent_Detection —Ç–æ–ª—å–∫–æ –≤ train")
    
    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º
    random.Random(seed).shuffle(train_records)
    random.Random(seed).shuffle(val_records)
    
    # –ü–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
    def summarize_records(records, title):
        from collections import Counter
        by_src = Counter(rec['source'] for rec in records)
        by_cls = Counter(rec['label'] for rec in records)
        
        print(f"\nüìà {title}:")
        print(f"   üìä –ü–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")
        for source, count in sorted(by_src.items()):
            percentage = (count / len(records) * 100) if len(records) > 0 else 0
            print(f"      ‚Ä¢ {source}: {count} ({percentage:.1f}%)")
        
        print(f"   üìä –ü–æ –∫–ª–∞—Å—Å–∞–º:")
        for class_id in [0, 1, 2]:
            class_name = ["no_damage", "minor_damage", "major_damage"][class_id]
            count = by_cls[class_id]
            percentage = (count / len(records) * 100) if len(records) > 0 else 0
            print(f"      ‚Ä¢ {class_id} ({class_name}): {count} ({percentage:.1f}%)")
    
    # –ü–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    summarize_records(train_records, "TRAIN sources/classes")
    summarize_records(val_records, "VAL sources/classes")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\nüìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   Train: {len(train_records)} –∑–∞–ø–∏—Å–µ–π")
    print(f"   Validation: {len(val_records)} –∑–∞–ø–∏—Å–µ–π")
    
    # –ü–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    summarize_records(train_records, "TRAIN sources/classes")
    summarize_records(val_records, "VAL sources/classes")
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç (path, label) –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    train_items = [(rec['path'], rec['label']) for rec in train_records]
    val_items = [(rec['path'], rec['label']) for rec in val_records]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–ø–∏—Å–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ sampler
    return train_items, val_items, train_records, val_records

def split_data_stratified_internal(items, val_ratio=0.3, seed=42):
    """
    –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–π –∫–ª–∞—Å—Å–æ–≤
    """
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–ª–∞—Å—Å–∞–º
    class_items = {}
    for item in items:
        class_id = item[1]
        if class_id not in class_items:
            class_items[class_id] = []
        class_items[class_id].append(item)
    
    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
    for class_id in class_items:
        random.Random(seed).shuffle(class_items[class_id])
    
    train_items = []
    val_items = []
    
    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ –¥–µ–ª–∞–µ–º split
    for class_id, class_data in class_items.items():
        n_val = int(len(class_data) * val_ratio)
        val_items.extend(class_data[:n_val])
        train_items.extend(class_data[n_val:])
    
    return train_items, val_items

def create_enhanced_sampler_weights(train_records, boost_no_damage=1.4):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ—Å–æ–≤ –¥–ª—è —Å—ç–º–ø–ª–µ—Ä–∞ —Å —É—á–µ—Ç–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–∏–≥–ª—É—à–µ–Ω–∏–µ–º Dent_Detection
    """
    print("\nüéØ –°–û–ó–î–ê–ù–ò–ï –£–õ–£–ß–®–ï–ù–ù–´–• –í–ï–°–û–í –î–õ–Ø –°–≠–ú–ü–õ–ï–†–ê")
    
    # –í–µ—Å–∞ –ø–æ –¥–∞—Ç–∞—Å–µ—Ç–∞–º (–ø—Ä–∏–≥–ª—É—à–∞–µ–º Dent_Detection)
    dataset_weights = {
        "rust and scrach.v1i.multiclass": 1.0,
        "car scratch and dent.v5i.multiclass": 1.0,
        "dent_detection.v1i.multiclass": 0.6,  # –ü—Ä–∏–≥–ª—É—à–∞–µ–º –ø–µ—Ä–µ–∫–æ—Å
    }
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ train labels –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –∫–ª–∞—Å—Å –≤–µ—Å–æ–≤
    train_labels = [rec['label'] for rec in train_records]
    
    # Effective Number weights
    class_counts = Counter(train_labels)
    cls_counts = np.array([class_counts[0], class_counts[1], class_counts[2]], dtype=np.float32)
    
    print(f"   üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ train:")
    total_train = len(train_labels)
    for i, count in enumerate(cls_counts):
        class_name = ["no_damage", "minor_damage", "major_damage"][i]
        percentage = (count / total_train * 100)
        print(f"      {i} ({class_name}): {count:.0f} ({percentage:.1f}%)")
    
    # Effective Number calculation
    beta = 0.9999
    effective_num = 1.0 - np.power(beta, cls_counts)
    weights = (1.0 - beta) / np.array(effective_num)
    
    # –ë—É—Å—Ç –¥–ª—è no_damage –∫–ª–∞—Å—Å–∞
    weights[0] *= boost_no_damage
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Å–∞
    weights = weights / weights.sum() * len(weights)
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–±—Ä–æ—Å –≤–µ—Å–æ–≤ (max/min ‚â§ 12√ó)
    max_ratio = 12.0
    weight_ratio = weights.max() / weights.min()
    if weight_ratio > max_ratio:
        print(f"   ‚ö†Ô∏è  –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–±—Ä–æ—Å –≤–µ—Å–æ–≤: {weight_ratio:.2f}√ó ‚Üí {max_ratio:.2f}√ó")
        weights = np.clip(weights, weights.min(), weights.min() * max_ratio)
        weights = weights / weights.sum() * len(weights)
    
    print(f"   üéØ Effective Number –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤:")
    for i, weight in enumerate(weights):
        class_name = ["no_damage", "minor_damage", "major_damage"][i]
        print(f"      {i} ({class_name}): {weight:.2f}")
    
    print(f"   üéØ –í–µ—Å–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")
    for source, weight in dataset_weights.items():
        print(f"      {source}: {weight:.1f}")
    
    # –°–æ–∑–¥–∞–µ–º sample weights
    sample_weights = []
    
    for rec in train_records:
        class_weight = weights[rec['label']]
        
        # –ù–∞—Ö–æ–¥–∏–º –ø–æ–¥—Ö–æ–¥—è—â–∏–π dataset weight
        dataset_weight = 1.0
        for source_key, dw in dataset_weights.items():
            if source_key in rec['source']:
                dataset_weight = dw
                break
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ—Å
        combined_weight = class_weight * dataset_weight
        sample_weights.append(combined_weight)
    
    print(f"   ‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(sample_weights)} sample weights")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–µ—Å–æ–≤
    sample_weights = np.array(sample_weights)
    print(f"   üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ sample weights:")
    print(f"      Min: {sample_weights.min():.3f}")
    print(f"      Max: {sample_weights.max():.3f}")
    print(f"      Mean: {sample_weights.mean():.3f}")
    print(f"      Ratio: {sample_weights.max()/sample_weights.min():.2f}√ó")
    
    return sample_weights, weights

def create_effective_number_weights(labels, boost_no_damage=1.3):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Effective Number of Samples
    + –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –±—É—Å—Ç –¥–ª—è no_damage –∫–ª–∞—Å—Å–∞
    """
    class_counts = Counter(labels)
    cls_counts = np.array([class_counts[0], class_counts[1], class_counts[2]], dtype=np.float32)
    
    print(f"üìä Effective Number Weights —Ä–∞—Å—á–µ—Ç:")
    print(f"   –ò–∑–Ω–∞—á–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {cls_counts}")
    
    # Effective Number (Œ≤ –±–ª–∏–∑–∫–æ –∫ 1 –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–≤)
    beta = 0.9999
    eff_num = (1 - np.power(beta, cls_counts)) / (1 - beta)
    weights = eff_num.sum() / eff_num
    
    print(f"   Effective Numbers: {eff_num}")
    print(f"   –í–µ—Å–∞ –¥–æ –±—É—Å—Ç–∏–Ω–≥–∞: {weights}")
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –±—É—Å—Ç–∏–º –∫–ª–∞—Å—Å no_damage (–∫–ª–∞—Å—Å 0)
    weights[0] *= boost_no_damage
    
    print(f"   –ë—É—Å—Ç –¥–ª—è no_damage: √ó{boost_no_damage}")
    print(f"   –§–∏–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞: {weights}")
    
    return torch.tensor(weights, dtype=torch.float32)

def create_weighted_sampler(labels):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å—ç–º–ø–ª–µ—Ä–∞ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤ (–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π)"""
    class_counts = Counter(labels)
    total_samples = len(labels)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ (–æ–±—Ä–∞—Ç–Ω–æ –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —á–∞—Å—Ç–æ—Ç–µ)
    class_weights = {}
    for class_id, count in class_counts.items():
        class_weights[class_id] = total_samples / (len(class_counts) * count)
    
    # –°–æ–∑–¥–∞–µ–º –≤–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–º–ø–ª–∞
    sample_weights = [class_weights[label] for label in labels]
    
    print(f"üìä –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —Å—ç–º–ø–ª–µ—Ä–∞:")
    for class_id in sorted(class_weights.keys()):
        class_name = ["no_damage", "minor_damage", "major_damage"][class_id]
        weight = class_weights[class_id]
        count = class_counts[class_id]
        print(f"   {class_id} ({class_name}): {weight:.3f} (—Å–µ–º–ø–ª–æ–≤: {count})")
    
    return WeightedRandomSampler(
        weights=sample_weights,
        num_samples=len(sample_weights),
        replacement=True
    )

def calculate_metrics(y_true, y_pred, y_scores=None):
    """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞"""
    metrics = {}
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    metrics['accuracy'] = (y_true == y_pred).mean()
    metrics['macro_f1'] = f1_score(y_true, y_pred, average='macro')
    metrics['weighted_f1'] = f1_score(y_true, y_pred, average='weighted')
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º
    f1_per_class = f1_score(y_true, y_pred, average=None)
    class_names = ["no_damage", "minor_damage", "major_damage"]
    
    for i, class_name in enumerate(class_names):
        metrics[f'f1_{class_name}'] = f1_per_class[i]
    
    return metrics

def save_confusion_matrix(y_true, y_pred, save_path):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫"""
    cm = confusion_matrix(y_true, y_pred)
    class_names = ["no_damage", "minor_damage", "major_damage"]
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()

def train_epoch(model, dataloader, criterion, optimizer, device, clip_grad_norm=1.0):
    """–¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏ —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º –∫–ª–∏–ø–ø–∏–Ω–≥–æ–º"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    pbar = tqdm(dataloader, desc="Training")
    for batch_idx, (images, labels) in enumerate(pbar):
        images, labels = images.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        
        # –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–µ –∫–ª–∏–ø–ø–∏—Ä–æ–≤–∞–Ω–∏–µ (–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ —Å FocalLoss + class weights)
        if clip_grad_norm > 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)
        
        optimizer.step()
        
        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        if batch_idx % 10 == 0:
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*correct/total:.2f}%'
            })
    
    return total_loss / len(dataloader), correct / total

def save_confusion_matrix(y_true, y_pred, save_path):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç confusion matrix"""
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['no_damage', 'minor_damage', 'major_damage'],
                yticklabels=['no_damage', 'minor_damage', 'major_damage'])
    plt.title('Confusion Matrix (Best Model)')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()

def validate_epoch(model, dataloader, criterion, device):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏"""
    model.eval()
    total_loss = 0
    y_true = []
    y_pred = []
    y_scores = []
    
    with torch.no_grad():
        for images, labels in tqdm(dataloader, desc="Validation"):
            images, labels = images.to(device), labels.to(device)
            
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –º–µ—Ç—Ä–∏–∫
            y_true.extend(labels.cpu().numpy())
            y_pred.extend(predicted.cpu().numpy())
            y_scores.extend(F.softmax(outputs, dim=1).cpu().numpy())
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    metrics = calculate_metrics(np.array(y_true), np.array(y_pred), np.array(y_scores))
    
    return total_loss / len(dataloader), metrics, y_true, y_pred

def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, 
                num_epochs, device, save_dir, patience=7, freeze_backbone_epochs=0):
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–æ–π"""
    
    best_val_f1 = 0
    bad_epochs = 0
    train_losses = []
    val_losses = []
    train_accs = []
    val_f1s = []
    
    print(f"\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {num_epochs} —ç–ø–æ—Ö")
    print(f"üîß –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    print(f"‚è∞ Early stopping patience: {patience}")
    if freeze_backbone_epochs > 0:
        print(f"üßä –ó–∞–º–æ—Ä–æ–∑–∫–∞ backbone –Ω–∞ {freeze_backbone_epochs} —ç–ø–æ—Ö")
    
    for epoch in range(num_epochs):
        print(f"\n{'='*50}")
        print(f"–≠–ø–æ—Ö–∞ {epoch+1}/{num_epochs}")
        print(f"{'='*50}")
        
        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –∑–∞–º–æ—Ä–æ–∑–∫–∞ backbone –≤ –Ω–∞—á–∞–ª–µ
        if epoch < freeze_backbone_epochs:
            for param in model.backbone.parameters():
                param.requires_grad = False
            print("üßä Backbone –∑–∞–º–æ—Ä–æ–∂–µ–Ω - –æ–±—É—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ classifier")
        elif epoch == freeze_backbone_epochs and freeze_backbone_epochs > 0:
            for param in model.backbone.parameters():
                param.requires_grad = True
            print("üî• Backbone —Ä–∞–∑–º–æ—Ä–æ–∂–µ–Ω - –æ–±—É—á–∞–µ—Ç—Å—è –≤—Å—è –º–æ–¥–µ–ª—å")
        
        # –û–±—É—á–µ–Ω–∏–µ
        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        val_loss, val_metrics, y_true, y_pred = validate_epoch(model, val_loader, criterion, device)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º learning rate (–≤–∞–∂–Ω–æ: –ø–æ macro F1, –Ω–µ –ø–æ loss!)
        scheduler.step(val_metrics['macro_f1'])
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_accs.append(train_acc)
        val_f1s.append(val_metrics['macro_f1'])
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–ø–æ—Ö–∏ {epoch+1}:")
        print(f"   Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}")
        print(f"   Val Loss: {val_loss:.4f} | Val F1 (macro): {val_metrics['macro_f1']:.4f}")
        print(f"   Val F1 (weighted): {val_metrics['weighted_f1']:.4f}")
        print(f"   Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
        
        # F1-score –ø–æ –∫–ª–∞—Å—Å–∞–º (–ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û –¥–ª—è imbalanced data!)
        print(f"   F1 –ø–æ –∫–ª–∞—Å—Å–∞–º:")
        for class_name in ["no_damage", "minor_damage", "major_damage"]:
            f1_val = val_metrics[f'f1_{class_name}']
            print(f"     {class_name}: {f1_val:.4f}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–ª—É—á—à–µ–Ω–∏–µ
        if val_metrics['macro_f1'] > best_val_f1:
            best_val_f1 = val_metrics['macro_f1']
            bad_epochs = 0
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_f1': best_val_f1,
                'val_metrics': val_metrics
            }, save_dir / 'best_model.pth')
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º confusion matrix –¥–ª—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            save_confusion_matrix(y_true, y_pred, save_dir / 'confusion_matrix.png')
            
            print(f"   ‚úÖ –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å! F1: {best_val_f1:.4f}")
        else:
            bad_epochs += 1
            print(f"   üìâ –ë–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è: {bad_epochs}/{patience}")
        
        # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ macro F1 (–ù–ï –ø–æ loss!)
        if bad_epochs >= patience:
            print(f"\n‚è∞ –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
            print(f"   –õ—É—á—à–∏–π macro F1: {best_val_f1:.4f}")
            break
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ overfitting
        if len(train_losses) > 1:
            train_val_gap = train_acc - val_metrics['accuracy']
            if train_val_gap > 0.2:  # 20% gap
                print(f"   ‚ö†Ô∏è –ü—Ä–∏–∑–Ω–∞–∫–∏ overfitting: gap = {train_val_gap:.3f}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
    save_training_plots(train_losses, val_losses, train_accs, val_f1s, save_dir)
    
    return best_val_f1

def save_training_plots(train_losses, val_losses, train_accs, val_f1s, save_dir):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # Loss
    ax1.plot(train_losses, label='Train Loss')
    ax1.plot(val_losses, label='Validation Loss')
    ax1.set_title('Training and Validation Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True)
    
    # Accuracy vs F1
    ax2.plot(train_accs, label='Train Accuracy')
    ax2.plot(val_f1s, label='Validation F1')
    ax2.set_title('Training Accuracy vs Validation F1')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Score')
    ax2.legend()
    ax2.grid(True)
    
    # Learning curves
    ax3.plot(range(len(train_losses)), train_losses, 'b-', alpha=0.7, label='Train')
    ax3.plot(range(len(val_losses)), val_losses, 'r-', alpha=0.7, label='Validation')
    ax3.set_title('Learning Curves')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('Loss')
    ax3.legend()
    ax3.grid(True)
    
    # F1 progression
    ax4.plot(val_f1s, 'g-', linewidth=2)
    ax4.set_title('Validation F1 Score Progress')
    ax4.set_xlabel('Epoch')
    ax4.set_ylabel('F1 Score')
    ax4.grid(True)
    
    plt.tight_layout()
    plt.savefig(save_dir / 'training_plots.png', dpi=150)
    plt.close()

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    print("ü§ñ –°–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π")
    print("="*60)
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    save_dir = Path("training_results")
    save_dir.mkdir(exist_ok=True)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
    print("\n1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π")
    train_items, val_items, train_records, val_records = proper_dataset_split(DATASET_ROOTS, val_ratio=0.3)
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã
    if len(train_items) == 0:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è!")
        return
    
    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    print("\n2Ô∏è‚É£ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π")
    train_transforms = create_training_transforms()
    val_transforms = create_validation_transforms()
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
    train_dataset = MulticlassDamageDataset(train_items, train_transforms)
    val_dataset = MulticlassDamageDataset(val_items, val_transforms)
    
    # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π —Å—ç–º–ø–ª–µ—Ä —Å —É—á–µ—Ç–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    print("\n3Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ —Å—ç–º–ø–ª–µ—Ä–∞ —Å —É—á–µ—Ç–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    sample_weights, class_weights = create_enhanced_sampler_weights(train_records, boost_no_damage=1.4)
    
    sampler = WeightedRandomSampler(
        weights=sample_weights,
        num_samples=len(sample_weights),
        replacement=True
    )
    
    # –°–æ–∑–¥–∞–µ–º DataLoader'—ã —Å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
    batch_size = 16  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è CPU
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size,
        sampler=sampler,
        num_workers=2,  # –î–ª—è CPU –Ω–µ –±–æ–ª—å—à–µ
        pin_memory=False,  # –ù–µ –Ω—É–∂–Ω–æ –±–µ–∑ GPU
        drop_last=True  # –ö–†–ò–¢–ò–ß–ù–û: –∏–∑–±–µ–≥–∞–µ–º BN –æ—à–∏–±–∫—É –Ω–∞ batch_size=1
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=64,  # –î–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–∂–Ω–æ –±–æ–ª—å—à–µ
        shuffle=False,
        num_workers=2,
        pin_memory=False
    )
    
    print(f"   Batch size: {batch_size}")
    print(f"   Train batches: {len(train_loader)}")
    print(f"   Validation batches: {len(val_loader)}")
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    print("\n4Ô∏è‚É£ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏")
    model = MulticlassDamageModel(num_classes=3, dropout=0.5)
    model = model.to(DEVICE)
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"   –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
    print(f"   –û–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {trainable_params:,}")
    
    # –°–æ–∑–¥–∞–µ–º FocalLoss —Å –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
    print("\n4Ô∏è‚É£ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ loss —Ñ—É–Ω–∫—Ü–∏–∏")
    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)
    
    # FocalLoss —Å gamma=2.0 (–º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å 2.5 –µ—Å–ª–∏ no_damage F1 –Ω–∏–∑–∫–∏–π)
    criterion = FocalLoss(alpha=class_weights_tensor, gamma=2.0)
    
    print(f"   –§–∏–Ω–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è FocalLoss: {class_weights}")
    print(f"   Gamma: 2.0 (–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞)")
    print(f"   ‚úÖ Loss —É—á–∏—Ç—ã–≤–∞–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –∫–ª–∞—Å—Å –¥–∏—Å–±–∞–ª–∞–Ω—Å")
    
    # –î–ò–§–§–ï–†–ï–ù–¶–ò–†–û–í–ê–ù–ù–´–ô LEARNING RATE (–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω–æ!)
    print("\n5Ô∏è‚É£ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞")
    backbone_lr = 1e-5  # –û—Å—Ç–æ—Ä–æ–∂–Ω–æ —Å pretrained weights
    classifier_lr = 1e-4  # –ù–æ–≤—ã–µ —Å–ª–æ–∏ –±—ã—Å—Ç—Ä–µ–µ
    
    optimizer = torch.optim.AdamW([
        {"params": model.backbone.parameters(), "lr": backbone_lr},
        {"params": model.classifier.parameters(), "lr": classifier_lr},
    ], weight_decay=5e-4)  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 1e-4 –¥–æ 5e-4 –ø—Ä–æ—Ç–∏–≤ overfitting
    
    print(f"   Backbone LR: {backbone_lr} (–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ —Å pretrained)")
    print(f"   Classifier LR: {classifier_lr} (–Ω–æ–≤—ã–µ —Å–ª–æ–∏)")
    print(f"   Weight decay: 5e-4 (—É—Å–∏–ª–µ–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ç–∏–≤ overfitting)")
    
    # Scheduler –ø–æ macro F1 (–ù–ï –ø–æ loss!) —Å —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å—é –≤–µ—Ä—Å–∏–π
    try:
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='max', factor=0.5, patience=3, min_lr=1e-7, verbose=True
        )
    except TypeError:
        # –°—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ PyTorch –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç verbose
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='max', factor=0.5, patience=3, min_lr=1e-7
        )
    
    print(f"   Scheduler: ReduceLROnPlateau –ø–æ macro-F1")
    print(f"   Factor: 0.5, Patience: 3, Min LR: 1e-7")
    
    # –û–±—É—á–µ–Ω–∏–µ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
    print("\n6Ô∏è‚É£ –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å —Ç—Ä–µ–º—è –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏")
    num_epochs = 30  # –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ 30 —ç–ø–æ—Ö
    freeze_backbone_epochs = 3  # –ó–∞–º–æ—Ä–æ–∑–∫–∞ backbone –Ω–∞ 3 —ç–ø–æ—Ö–∏
    
    best_f1 = train_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        num_epochs=num_epochs,
        device=DEVICE,
        save_dir=save_dir,
        patience=10,  # Early stopping –ø–æ—Å–ª–µ 10 —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è
        freeze_backbone_epochs=freeze_backbone_epochs
    )
    
    print(f"\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"   –õ—É—á—à–∏–π macro F1-score: {best_f1:.4f}")
    print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {save_dir}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    total_images = len(train_items) + len(val_items)
    train_labels = [item[1] for item in train_items]
    val_labels = [item[1] for item in val_items]
    combined_labels = train_labels + val_labels
    
    final_stats = {
        'dataset_info': {
            'total_images': total_images,
            'train_images': len(train_items),
            'val_images': len(val_items),
            'class_distribution': dict(Counter(combined_labels))
        },
        'training_info': {
            'device': DEVICE,
            'batch_size': batch_size,
            'backbone_lr': backbone_lr,
            'classifier_lr': classifier_lr,
            'num_epochs': num_epochs,
            'best_f1': float(best_f1),
            'gamma': 2.0,
            'boost_no_damage': 1.3
        }
    }
    
    with open(save_dir / 'training_stats.json', 'w', encoding='utf-8') as f:
        json.dump(final_stats, f, indent=2, ensure_ascii=False)

if __name__ == "__main__":
    # –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å
    torch.manual_seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)
    random.seed(RANDOM_SEED)
    
    main()