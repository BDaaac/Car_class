"""
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –º–∞—Å—à—Ç–∞–±–Ω–æ–≥–æ –Ω–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å —á–∏—Å—Ç—ã–º–∏ –∏ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–º–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª—è–º–∏
1700 —á–∏—Å—Ç—ã—Ö + 400+ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö = —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞!
"""
import os
import shutil
from pathlib import Path
import pandas as pd
from typing import List, Dict
import json

def create_dataset_structure(dataset_root: str):
    """–°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫ –¥–ª—è –Ω–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    base_path = Path(dataset_root)
    
    # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫
    folders = [
        "train/no_damage",
        "train/minor_damage", 
        "train/major_damage"
    ]
    
    for folder in folders:
        folder_path = base_path / folder
        folder_path.mkdir(parents=True, exist_ok=True)
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞: {folder_path}")
    
    return base_path

def organize_new_dataset(
    clean_cars_path: str,
    damaged_cars_path: str, 
    output_dataset_root: str
):
    """
    –û—Ä–≥–∞–Ω–∏–∑—É–µ—Ç –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä—É multiclass
    
    Args:
        clean_cars_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å 1700 —á–∏—Å—Ç—ã–º–∏ –º–∞—à–∏–Ω–∞–º–∏
        damaged_cars_path: –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å 400+ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–º–∏ –º–∞—à–∏–Ω–∞–º–∏
        output_dataset_root: –ö–æ—Ä–Ω–µ–≤–∞—è –ø–∞–ø–∫–∞ –¥–ª—è –Ω–æ–≤–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    """
    
    print("üöÄ –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –ú–ê–°–®–¢–ê–ë–ù–û–ì–û –î–ê–¢–ê–°–ï–¢–ê")
    print("="*50)
    
    # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    base_path = create_dataset_structure(output_dataset_root)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = {
        "clean_copied": 0,
        "damaged_copied": 0,
        "errors": []
    }
    
    # –ö–æ–ø–∏—Ä—É–µ–º —á–∏—Å—Ç—ã–µ –º–∞—à–∏–Ω—ã –≤ no_damage
    print(f"\n1Ô∏è‚É£ –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å—Ç—ã—Ö –º–∞—à–∏–Ω –∏–∑ {clean_cars_path}")
    clean_target = base_path / "train" / "no_damage"
    
    if os.path.exists(clean_cars_path):
        for filename in os.listdir(clean_cars_path):
            if filename.lower().endswith(('.jpg', '.jpeg', '.png')):
                try:
                    src = os.path.join(clean_cars_path, filename)
                    dst = clean_target / f"clean_{stats['clean_copied']:04d}_{filename}"
                    shutil.copy2(src, dst)
                    stats['clean_copied'] += 1
                    
                    if stats['clean_copied'] % 100 == 0:
                        print(f"   –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ: {stats['clean_copied']} —á–∏—Å—Ç—ã—Ö –º–∞—à–∏–Ω...")
                        
                except Exception as e:
                    stats['errors'].append(f"–û—à–∏–±–∫–∞ —Å {filename}: {e}")
                    
        print(f"‚úÖ –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ {stats['clean_copied']} —á–∏—Å—Ç—ã—Ö –º–∞—à–∏–Ω")
    else:
        print(f"‚ùå –ü–∞–ø–∫–∞ {clean_cars_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
    
    # –ö–æ–ø–∏—Ä—É–µ–º –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–µ –º–∞—à–∏–Ω—ã
    print(f"\n2Ô∏è‚É£ –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö –º–∞—à–∏–Ω –∏–∑ {damaged_cars_path}")
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–µ –Ω–∞ minor –∏ major (50/50)
    damaged_target_minor = base_path / "train" / "minor_damage"
    damaged_target_major = base_path / "train" / "major_damage"
    
    if os.path.exists(damaged_cars_path):
        damaged_files = [f for f in os.listdir(damaged_cars_path) 
                        if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        
        for i, filename in enumerate(damaged_files):
            try:
                src = os.path.join(damaged_cars_path, filename)
                
                # –ß–µ—Ä–µ–¥—É–µ–º minor/major –¥–ª—è –±–∞–ª–∞–Ω—Å–∞
                if i % 2 == 0:
                    dst = damaged_target_minor / f"minor_{i:04d}_{filename}"
                else:
                    dst = damaged_target_major / f"major_{i:04d}_{filename}"
                
                shutil.copy2(src, dst)
                stats['damaged_copied'] += 1
                
                if stats['damaged_copied'] % 50 == 0:
                    print(f"   –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ: {stats['damaged_copied']} –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö –º–∞—à–∏–Ω...")
                    
            except Exception as e:
                stats['errors'].append(f"–û—à–∏–±–∫–∞ —Å {filename}: {e}")
                
        print(f"‚úÖ –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ {stats['damaged_copied']} –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã—Ö –º–∞—à–∏–Ω")
        print(f"   Minor damage: ~{stats['damaged_copied']//2}")
        print(f"   Major damage: ~{stats['damaged_copied']//2}")
    else:
        print(f"‚ùå –ü–∞–ø–∫–∞ {damaged_cars_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
    
    # –°–æ–∑–¥–∞–µ–º CSV –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
    print(f"\n3Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ CSV –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π...")
    create_csv_annotations(base_path, stats)
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   –ß–∏—Å—Ç—ã–µ –º–∞—à–∏–Ω—ã: {stats['clean_copied']}")
    print(f"   –ü–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–µ –º–∞—à–∏–Ω—ã: {stats['damaged_copied']}")
    print(f"   –í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {stats['clean_copied'] + stats['damaged_copied']}")
    print(f"   –û—à–∏–±–∫–∏: {len(stats['errors'])}")
    
    if stats['errors']:
        print(f"\n‚ö†Ô∏è –û—à–∏–±–∫–∏:")
        for error in stats['errors'][:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
            print(f"   {error}")
    
    return stats

def create_csv_annotations(base_path: Path, stats: Dict):
    """–°–æ–∑–¥–∞–µ—Ç CSV —Ñ–∞–π–ª—ã –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
    
    annotations = []
    
    # no_damage
    no_damage_path = base_path / "train" / "no_damage"
    for img_file in no_damage_path.glob("*.jpg"):
        annotations.append({
            "filename": img_file.name,
            "class": "no_damage",
            "label": 0,
            "source": "new_massive_dataset"
        })
    
    for img_file in no_damage_path.glob("*.png"):
        annotations.append({
            "filename": img_file.name,
            "class": "no_damage", 
            "label": 0,
            "source": "new_massive_dataset"
        })
    
    # minor_damage
    minor_damage_path = base_path / "train" / "minor_damage"
    for img_file in minor_damage_path.glob("*.jpg"):
        annotations.append({
            "filename": img_file.name,
            "class": "minor_damage",
            "label": 1,
            "source": "new_massive_dataset"
        })
        
    for img_file in minor_damage_path.glob("*.png"):
        annotations.append({
            "filename": img_file.name,
            "class": "minor_damage",
            "label": 1,
            "source": "new_massive_dataset"
        })
    
    # major_damage
    major_damage_path = base_path / "train" / "major_damage"
    for img_file in major_damage_path.glob("*.jpg"):
        annotations.append({
            "filename": img_file.name,
            "class": "major_damage",
            "label": 2,
            "source": "new_massive_dataset"
        })
        
    for img_file in major_damage_path.glob("*.png"):
        annotations.append({
            "filename": img_file.name,
            "class": "major_damage",
            "label": 2,
            "source": "new_massive_dataset"
        })
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV
    df = pd.DataFrame(annotations)
    csv_path = base_path / "annotations.csv"
    df.to_csv(csv_path, index=False)
    
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π: {csv_path}")
    print(f"   –ó–∞–ø–∏—Å–µ–π –≤ CSV: {len(annotations)}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats_path = base_path / "dataset_stats.json"
    distribution = df['class'].value_counts().to_dict()
    
    dataset_info = {
        "total_images": len(annotations),
        "class_distribution": distribution,
        "source": "new_massive_dataset_integration",
        "created_by": "dataset_integration_script",
        "notes": "1700 clean cars + 400+ damaged cars integration"
    }
    
    with open(stats_path, 'w') as f:
        json.dump(dataset_info, f, indent=2)
    
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {stats_path}")

def update_training_script_paths():
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –ø—É—Ç–∏ –≤ train_multiclass_damage.py"""
    
    new_dataset_path = r"C:\Users\–î–∏–º–∞—à\Desktop\python\hackaton\data\New_Massive_Dataset.v1i.multiclass\train"
    
    print(f"\n4Ô∏è‚É£ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—É—Ç–µ–π –≤ —Å–∫—Ä–∏–ø—Ç–µ –æ–±—É—á–µ–Ω–∏—è...")
    print(f"   –ù–æ–≤—ã–π –ø—É—Ç—å: {new_dataset_path}")
    
    # TODO: –ú–æ–∂–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–∏—Ç—å DATASET_ROOTS –≤ train_multiclass_damage.py
    
    recommended_paths = [
        r"C:\Users\–î–∏–º–∞—à\Desktop\python\hackaton\data\Rust and Scrach.v1i.multiclass\train",
        r"C:\Users\–î–∏–º–∞—à\Desktop\python\hackaton\data\Car Scratch and Dent.v5i.multiclass\train",
        r"C:\Users\–î–∏–º–∞—à\Desktop\python\hackaton\data\Dent_Detection.v1i.multiclass\train",
        new_dataset_path  # –ù–æ–≤—ã–π!
    ]
    
    print(f"\nüìù –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ï –ü–£–¢–ò –î–õ–Ø DATASET_ROOTS:")
    for i, path in enumerate(recommended_paths, 1):
        print(f"   {i}. {path}")
    
    return recommended_paths

def analyze_combined_dataset_balance():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –±–∞–ª–∞–Ω—Å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    
    print(f"\nüìä –ê–ù–ê–õ–ò–ó –ë–ê–õ–ê–ù–°–ê –û–ë–™–ï–î–ò–ù–ï–ù–ù–û–ì–û –î–ê–¢–ê–°–ï–¢–ê")
    print("="*50)
    
    # –¢–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ
    current_data = {
        "no_damage": 41,
        "minor_damage": 278, 
        "major_damage": 331
    }
    
    # –ù–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ (–æ—Ü–µ–Ω–∫–∞)
    new_data = {
        "no_damage": 1700,
        "minor_damage": 200,  # ~50% –æ—Ç 400
        "major_damage": 200   # ~50% –æ—Ç 400
    }
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    combined_data = {}
    for key in current_data:
        combined_data[key] = current_data[key] + new_data[key]
    
    total = sum(combined_data.values())
    
    print(f"–¢–ï–ö–£–©–ò–ï –î–ê–ù–ù–´–ï:")
    for cls, count in current_data.items():
        percent = (count / sum(current_data.values())) * 100
        print(f"   {cls}: {count} ({percent:.1f}%)")
    
    print(f"\n–ù–û–í–´–ï –î–ê–ù–ù–´–ï:")
    for cls, count in new_data.items():
        percent = (count / sum(new_data.values())) * 100
        print(f"   {cls}: {count} ({percent:.1f}%)")
    
    print(f"\n–û–ë–™–ï–î–ò–ù–ï–ù–ù–´–ï –î–ê–ù–ù–´–ï:")
    for cls, count in combined_data.items():
        percent = (count / total) * 100
        print(f"   {cls}: {count} ({percent:.1f}%)")
    
    print(f"\n–û–ë–©–ò–ô –†–ê–ó–ú–ï–†: {total} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    # –ê–Ω–∞–ª–∏–∑ –±–∞–ª–∞–Ω—Å–∞
    max_count = max(combined_data.values())
    min_count = min(combined_data.values())
    imbalance_ratio = max_count / min_count
    
    print(f"\n–ë–ê–õ–ê–ù–° –ö–õ–ê–°–°–û–í:")
    print(f"   –î–∏—Å–±–∞–ª–∞–Ω—Å: {imbalance_ratio:.2f}:1")
    
    if imbalance_ratio < 3:
        print(f"   ‚úÖ –•–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤!")
    elif imbalance_ratio < 5:
        print(f"   üü° –£–º–µ—Ä–µ–Ω–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å")
    else:
        print(f"   ‚ùå –°–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å")
    
    return combined_data

if __name__ == "__main__":
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—É—Ç–µ–π (–û–ë–ù–û–í–ò –≠–¢–ò –ü–£–¢–ò!)
    CLEAN_CARS_PATH = r"C:\Users\–î–∏–º–∞—à\Desktop\python\hackaton\new_data\clean_cars"
    DAMAGED_CARS_PATH = r"C:\Users\–î–∏–º–∞—à\Desktop\python\hackaton\new_data\damaged_cars"
    OUTPUT_DATASET = r"C:\Users\–î–∏–º–∞—à\Desktop\python\hackaton\data\New_Massive_Dataset.v1i.multiclass"
    
    print(f"üìÅ –ü–£–¢–ò –î–õ–Ø –ò–ù–¢–ï–ì–†–ê–¶–ò–ò:")
    print(f"   –ß–∏—Å—Ç—ã–µ –º–∞—à–∏–Ω—ã: {CLEAN_CARS_PATH}")
    print(f"   –ü–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω—ã–µ –º–∞—à–∏–Ω—ã: {DAMAGED_CARS_PATH}")
    print(f"   –í—ã—Ö–æ–¥–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç: {OUTPUT_DATASET}")
    
    # –ê–Ω–∞–ª–∏–∑ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ –±–∞–ª–∞–Ω—Å–∞
    analyze_combined_dataset_balance()
    
    # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
    try:
        stats = organize_new_dataset(
            clean_cars_path=CLEAN_CARS_PATH,
            damaged_cars_path=DAMAGED_CARS_PATH,
            output_dataset_root=OUTPUT_DATASET
        )
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—É—Ç–µ–π
        recommended_paths = update_training_script_paths()
        
        print(f"\nüéâ –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
        print(f"   –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ {stats['clean_copied'] + stats['damaged_copied']} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏: {e}")
        import traceback
        traceback.print_exc()