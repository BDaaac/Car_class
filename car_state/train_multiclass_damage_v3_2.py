"""
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –º–æ–¥–µ–ª–∏
–§–∏–∫—Å–∏—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–æ–º –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∫—É –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
"""
import os
import re
import json
import random
import math
from pathlib import Path
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from torchvision import transforms
from PIL import Image
from collections import Counter
from sklearn.metrics import f1_score, roc_curve, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import sys

# –ò–º–ø–æ—Ä—Ç —Ñ—É–Ω–∫—Ü–∏–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –º–æ–¥—É–ª—è
try:
    from multiclass_damage_model import MulticlassDamageModel, FocalLoss, create_training_transforms, create_validation_transforms
except ImportError:
    print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π, —Å–æ–∑–¥–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –≤–µ—Ä—Å–∏–∏")
    
    def create_training_transforms():
        """–°–æ–∑–¥–∞–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        return transforms.Compose([
            # --- PIL stage (–¥–æ ToTensor) ---
            transforms.Resize((256, 256)),
            transforms.RandomResizedCrop(224, scale=(0.75, 1.0)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=15),
            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),
            
            # --- Tensor stage (–ø–æ—Å–ª–µ ToTensor) ---
            transforms.ToTensor(),  # –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –ø–µ—Ä–µ–¥ Normalize/RandomErasing
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), value='random'),
        ])
    
    def create_validation_transforms():
        """–°–æ–∑–¥–∞–µ—Ç —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
        return transforms.Compose([
            # --- PIL stage ---
            transforms.Resize((224, 224)),
            
            # --- Tensor stage ---
            transforms.ToTensor(),  # –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –ø–µ—Ä–µ–¥ Normalize
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    # –ü—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –∏–º–ø–æ—Ä—Ç –Ω–µ —É–¥–∞–ª—Å—è
    class MulticlassDamageModel(nn.Module):
        def __init__(self, num_classes=3):
            super().__init__()
            from torchvision.models import resnet50
            self.backbone = resnet50(pretrained=True)
            self.backbone.fc = nn.Sequential(
                nn.Dropout(0.6),
                nn.Linear(2048, 1024),
                nn.ReLU(),
                nn.BatchNorm1d(1024),
                nn.Dropout(0.3),
                nn.Linear(1024, 512),
                nn.ReLU(),
                nn.BatchNorm1d(512),
                nn.Dropout(0.15),
                nn.Linear(512, num_classes)
            )
        
        def forward(self, x):
            return self.backbone(x)

# –ò–º–ø–æ—Ä—Ç –Ω–∞—à–∏—Ö –º–æ–¥—É–ª–µ–π
try:
    from .multiclass_damage_model import MulticlassDamageModel, FocalLoss, create_training_transforms, create_validation_transforms
except ImportError:
    current_dir = os.path.dirname(os.path.abspath(__file__))
    if current_dir not in sys.path:
        sys.path.append(current_dir)
    from multiclass_damage_model import MulticlassDamageModel, FocalLoss, create_training_transforms, create_validation_transforms

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
RANDOM_SEED = 42

# –ü—É—Ç–∏ –∫ –¥–∞—Ç–∞—Å–µ—Ç–∞–º (–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞)
DATASET_ROOTS = [
    r"C:\Users\–î–∏–º–∞—à\Desktop\python\hackaton\data\Rust and Scrach.v1i.multiclass\train",
    r"C:\Users\–î–∏–º–∞—à\Desktop\python\hackaton\data\Car Scratch and Dent.v5i.multiclass\train",
    r"C:\Users\–î–∏–º–∞—à\Desktop\python\hackaton\data\Dent_Detection.v1i.multiclass\train",
    r"C:\Users\–î–∏–º–∞—à\Desktop\python\hackaton\data\integrated_multiclass_dataset",  # –ù–æ–≤—ã–π –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    r"C:\Users\–î–∏–º–∞—à\Desktop\python\hackaton\data\car  scratch.v2i.multiclass\train",  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    r"C:\Users\–î–∏–º–∞—à\Desktop\python\hackaton\data\Car damages.v3i.multiclass\train",  # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–º–µ—Ä–æ–≤
]

def set_seeds(seed=RANDOM_SEED):
    """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç seeds –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

def normalize_columns(df):
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ –≤ CSV"""
    df.columns = df.columns.str.lower().str.replace(' ', '_')
    
    # –ê–ª–∏–∞—Å—ã –¥–ª—è —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
    column_aliases = {
        'filename': 'image_path',
        'file': 'image_path', 
        'image': 'image_path',
        'path': 'image_path'
    }
    
    for old_name, new_name in column_aliases.items():
        if old_name in df.columns:
            df = df.rename(columns={old_name: new_name})
    
    return df

def convert_multilabel_to_multiclass(row, dataset_type):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç Multi-Label –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –≤ Multi-Class"""
    
    if dataset_type == "dent_detection":
        # –ü—Ä–æ—Å—Ç–∞—è –±–∏–Ω–∞—Ä–Ω–∞—è –ª–æ–≥–∏–∫–∞: dent=1 ‚Üí minor_damage, dent=0 ‚Üí no_damage
        dent_val = row.get('dent', 0)
        if dent_val == 1:
            return 1  # minor_damage
        else:
            return 0  # no_damage
    
    elif dataset_type == "rust_scratch":
        # Rust and Scratch –¥–∞—Ç–∞—Å–µ—Ç
        rust = row.get('rust', 0)
        scratch = row.get('scratch', 0) 
        dent = row.get('dent', 0)
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–∞—è –ª–æ–≥–∏–∫–∞
        if rust == 1:  # –†–∂–∞–≤—á–∏–Ω–∞ = —Å–µ—Ä—å–µ–∑–Ω–æ–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–µ
            return 2  # major_damage
        elif dent == 1:  # –í–º—è—Ç–∏–Ω–∞ = —Å–µ—Ä—å–µ–∑–Ω–æ–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–µ  
            return 2  # major_damage
        elif scratch == 1:  # –¶–∞—Ä–∞–ø–∏–Ω–∞ = –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ
            return 1  # minor_damage
        else:
            return 0  # no_damage
            
    elif dataset_type == "car_scratch_dent":
        # Car Scratch and Dent –¥–∞—Ç–∞—Å–µ—Ç
        dent = row.get('dent', 0)
        scratch = row.get('scratch', 0)
        dirt = row.get('dirt', 0)
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–∞—è –ª–æ–≥–∏–∫–∞
        if dent == 1:  # –í–º—è—Ç–∏–Ω–∞ = —Å–µ—Ä—å–µ–∑–Ω–æ–µ
            return 2  # major_damage
        elif scratch == 1:  # –¶–∞—Ä–∞–ø–∏–Ω–∞ = –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ
            return 1  # minor_damage
        elif dirt == 1:  # –ì—Ä—è–∑—å = –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ (–º–æ–∂–Ω–æ –ø–æ—á–∏—Å—Ç–∏—Ç—å)
            return 1  # minor_damage  
        else:
            return 0  # no_damage
    
    elif dataset_type == "car_scratch_v2":
        # Car scratch.v2i.multiclass –¥–∞—Ç–∞—Å–µ—Ç (–Ω–æ–≤—ã–π)
        # –ö–æ–ª–æ–Ω–∫–∏: '0', 'scratch', 'car-scratch'
        zero_class = row.get('0', 0)  # –ö–ª–∞—Å—Å "0"  
        scratch = row.get('scratch', 0)  # –¶–∞—Ä–∞–ø–∏–Ω–∞
        car_scratch = row.get('car-scratch', 0)  # –ê–≤—Ç–æ–º–æ–±–∏–ª—å–Ω–∞—è —Ü–∞—Ä–∞–ø–∏–Ω–∞
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–∞—è –ª–æ–≥–∏–∫–∞: –µ—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Ç–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã, –≤—ã–±–∏—Ä–∞–µ–º –±–æ–ª–µ–µ —Å–µ—Ä—å–µ–∑–Ω—É—é
        if car_scratch == 1:  # car-scratch = —Å–µ—Ä—å–µ–∑–Ω–æ–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–µ
            return 2  # major_damage
        elif scratch == 1:  # scratch = –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–µ  
            return 1  # minor_damage
        elif zero_class == 1:  # –∫–ª–∞—Å—Å "0" = –±–µ–∑ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π
            return 0  # no_damage
        else:
            return 0  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é - –±–µ–∑ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π
    
    elif dataset_type == "car_scratch_v2":
        # Car scratch.v2i.multiclass –¥–∞—Ç–∞—Å–µ—Ç (–Ω–æ–≤—ã–π)
        # –ö–æ–ª–æ–Ω–∫–∏: '0', 'scratch', 'car-scratch'
        zero_class = row.get('0', 0)  # –ö–ª–∞—Å—Å "0"  
        scratch = row.get('scratch', 0)  # –¶–∞—Ä–∞–ø–∏–Ω–∞
        car_scratch = row.get('car-scratch', 0)  # –ê–≤—Ç–æ–º–æ–±–∏–ª—å–Ω–∞—è —Ü–∞—Ä–∞–ø–∏–Ω–∞
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–∞—è –ª–æ–≥–∏–∫–∞
        if car_scratch == 1:  # car-scratch = —Å–µ—Ä—å–µ–∑–Ω–æ–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–µ
            return 2  # major_damage
        elif scratch == 1:  # scratch = –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–µ
            return 1  # minor_damage  
        elif zero_class == 1:  # –∫–ª–∞—Å—Å "0" = –Ω–µ—Ç –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π
            return 0  # no_damage
        else:
            return 0  # no_damage (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    
    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
    return 0

def load_integrated_dataset_split(root_path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å —É–∂–µ –≥–æ—Ç–æ–≤—ã–º split'–æ–º train/test/valid
    """
    train_records = []
    val_records = []
    
    class_mapping = {
        "no_damage": 0,
        "minor_damage": 1, 
        "major_damage": 2
    }
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º train split
    train_path = root_path / "train"
    if train_path.exists():
        print("   üìÇ –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è train split")
        for class_name, class_id in class_mapping.items():
            class_path = train_path / class_name
            if not class_path.exists():
                continue
                
            image_files = []
            for ext in ["*.jpg", "*.jpeg", "*.png", "*.bmp"]:
                image_files.extend(class_path.glob(ext))
            
            for img_path in image_files:
                record = {
                    'path': str(img_path),
                    'label': int(class_id),
                    'source': 'integrated_dataset',
                    'dataset_type': 'integrated'
                }
                train_records.append(record)
            
            print(f"      {class_name}: {len(image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º valid split
    valid_path = root_path / "valid"
    if valid_path.exists():
        print("   üìÇ –ó–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è valid split")
        for class_name, class_id in class_mapping.items():
            class_path = valid_path / class_name
            if not class_path.exists():
                continue
                
            image_files = []
            for ext in ["*.jpg", "*.jpeg", "*.png", "*.bmp"]:
                image_files.extend(class_path.glob(ext))
            
            for img_path in image_files:
                record = {
                    'path': str(img_path),
                    'label': int(class_id),
                    'source': 'integrated_dataset',
                    'dataset_type': 'integrated'
                }
                val_records.append(record)
            
            print(f"      {class_name}: {len(image_files)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    print(f"   ‚úÖ Train: {len(train_records)}, Valid: {len(val_records)}")
    return train_records, val_records

def load_csv_dataset(root_path, csv_file):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ CSV-based –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    items = []
    class_distribution = Counter()
    
    try:
        df = pd.read_csv(csv_file)
        df = normalize_columns(df)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–∞—Ç–∞—Å–µ—Ç–∞
        columns = set(df.columns)
        if {'image_path', 'dent', 'dirt', 'scratch'}.issubset(columns):
            dataset_type = "car_scratch_dent"
        elif {'image_path', 'car', 'dent', 'rust', 'scratch'}.issubset(columns):
            dataset_type = "rust_scratch"  
        elif {'image_path', 'dent'}.issubset(columns) and len(columns) <= 3:
            dataset_type = "dent_detection"
        elif {'image_path', '0', 'scratch', 'car-scratch'}.issubset(columns):
            dataset_type = "car_scratch_v2"  # –ù–æ–≤—ã–π —Ç–∏–ø –¥–ª—è car scratch.v2i.multiclass
        else:
            print(f"   ‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç CSV. –ö–æ–ª–æ–Ω–∫–∏: {columns}")
            return items, class_distribution
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        for idx, row in df.iterrows():
            image_name = row['image_path']
            image_path = root_path / image_name
            
            if not image_path.exists():
                continue
            
            class_id = convert_multilabel_to_multiclass(row, dataset_type)
            items.append((str(image_path), class_id))
            class_distribution[class_id] += 1
        
        print(f"   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(items)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
    
    return items, class_distribution

def load_all_datasets(roots):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã"""
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    random.seed(RANDOM_SEED)
    
    all_train_records = []
    all_val_records = []
    
    print("üîç –ó–ê–ì–†–£–ó–ö–ê –í–°–ï–• –î–ê–¢–ê–°–ï–¢–û–í")
    
    for root in roots:
        root_path = Path(root)
        if not root_path.exists():
            print(f"‚ùå –ü—É—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω: {root_path}")
            continue
            
        print(f"\nüìÅ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {root_path.name}")
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        if "integrated_multiclass_dataset" in str(root_path):
            print("   üÜï –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç")
            train_records, val_records = load_integrated_dataset_split(root_path)
            all_train_records.extend(train_records)
            all_val_records.extend(val_records)
            continue
        
        # –û–±—ã—á–Ω—ã–µ CSV –¥–∞—Ç–∞—Å–µ—Ç—ã
        csv_files = list(root_path.glob("*.csv"))
        if not csv_files:
            print(f"   ‚ùå CSV –Ω–µ –Ω–∞–π–¥–µ–Ω")
            continue
            
        csv_file = csv_files[0]
        items, dist = load_csv_dataset(root_path, csv_file)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ records —Ñ–æ—Ä–º–∞—Ç
        dataset_name = root_path.parent.name.lower()
        for path, label in items:
            record = {
                'path': path,
                'label': label,
                'source': dataset_name,
                'dataset_type': 'csv'
            }
            # –î–ª—è Dent_Detection - —Ç–æ–ª—å–∫–æ –≤ train
            if "dent_detection" in dataset_name:
                all_train_records.append(record)
            else:
                # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –¥–µ–ª–∞–µ–º split
                if random.random() < 0.7:  # 70% –≤ train
                    all_train_records.append(record)
                else:
                    all_val_records.append(record)
    
    return all_train_records, all_val_records

class MulticlassDamageDataset(Dataset):
    """Dataset –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
    
    def __init__(self, records, transform=None):
        self.records = records
        self.transform = transform
    
    def __len__(self):
        return len(self.records)
    
    def __getitem__(self, idx):
        record = self.records[idx]
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        try:
            image = Image.open(record['path']).convert('RGB')  # –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û RGB
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {record['path']}: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —á–µ—Ä–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            image = Image.new('RGB', (224, 224), color=(0, 0, 0))
        
        # –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –ø—Ä–∏–º–µ–Ω—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
        if self.transform is not None:
            image = self.transform(image)  # PIL ‚Üí Tensor
        
        label = record['label']
        return image, label

def create_weighted_sampler(records):
    """–°–æ–∑–¥–∞–µ—Ç –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π —Å—ç–º–ø–ª–µ—Ä –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤"""
    labels = [r['label'] for r in records]
    label_counts = Counter(labels)
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–µ—Å–∞
    total = len(labels)
    weights = []
    
    for label in labels:
        weight = total / (len(label_counts) * label_counts[label])
        weights.append(weight)
    
    return WeightedRandomSampler(weights, len(weights), replacement=True)

def train_epoch(model, dataloader, criterion, optimizer, device):
    """–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏"""
    model.train()
    total_loss = 0
    all_preds = []
    all_labels = []
    
    progress_bar = tqdm(dataloader, desc="Training")
    
    for images, labels in progress_bar:
        images, labels = images.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        
        # Gradient clipping
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        total_loss += loss.item()
        _, preds = torch.max(outputs, 1)
        
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
        
        progress_bar.set_postfix(loss=loss.item())
    
    avg_loss = total_loss / len(dataloader)
    f1 = f1_score(all_labels, all_preds, average='macro')
    
    return avg_loss, f1

def validate_epoch(model, dataloader, criterion, device):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏"""
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for images, labels in tqdm(dataloader, desc="Validation"):
            images, labels = images.to(device), labels.to(device)
            
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            total_loss += loss.item()
            _, preds = torch.max(outputs, 1)
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    avg_loss = total_loss / len(dataloader)
    f1 = f1_score(all_labels, all_preds, average='macro')
    
    return avg_loss, f1

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""
    print("ü§ñ –û–ë–£–ß–ï–ù–ò–ï –ú–ù–û–ì–û–ö–õ–ê–°–°–û–í–û–ô –ú–û–î–ï–õ–ò v3.2")
    print("="*60)
    
    set_seeds(RANDOM_SEED)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    print("\n1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
    train_records, val_records = load_all_datasets(DATASET_ROOTS)
    
    if len(train_records) == 0:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        return
    
    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ê–ù–ù–´–•:")
    print(f"   Train: {len(train_records)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    print(f"   Valid: {len(val_records)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
    train_dist = Counter([r['label'] for r in train_records])
    val_dist = Counter([r['label'] for r in val_records])
    
    class_names = ["no_damage", "minor_damage", "major_damage"]
    
    print(f"\nüìà –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–°–û–í:")
    print(f"{'Class':<15} {'Train':<10} {'Valid':<10}")
    print("-" * 35)
    for i, name in enumerate(class_names):
        print(f"{name:<15} {train_dist[i]:<10} {val_dist[i]:<10}")
    
    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –æ—Ç–ª–∞–¥–∫–æ–π
    print("\n2Ô∏è‚É£ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π")
    
    train_transforms = create_training_transforms()
    val_transforms = create_validation_transforms()
    
    print(f"   ‚úÖ Train transforms: {len(train_transforms.transforms)} —ç—Ç–∞–ø–æ–≤")
    print(f"   ‚úÖ Val transforms: {len(val_transforms.transforms)} —ç—Ç–∞–ø–æ–≤")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä—è–¥–æ–∫
    for i, transform in enumerate(train_transforms.transforms):
        print(f"      {i+1}. {type(transform).__name__}")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã —Å –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û–ô –ø–µ—Ä–µ–¥–∞—á–µ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π
    train_dataset = MulticlassDamageDataset(train_records, transform=train_transforms)
    val_dataset = MulticlassDamageDataset(val_records, transform=val_transforms)
    
    print(f"   ‚úÖ Train dataset: {len(train_dataset)} –∑–∞–ø–∏—Å–µ–π")
    print(f"   ‚úÖ Val dataset: {len(val_dataset)} –∑–∞–ø–∏—Å–µ–π")
    
    # –°–æ–∑–¥–∞–µ–º —Å—ç–º–ø–ª–µ—Ä
    print("\n3Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–≥–æ —Å—ç–º–ø–ª–µ—Ä–∞")
    sampler = create_weighted_sampler(train_records)
    
    # DataLoader'—ã —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    batch_size = 16
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        sampler=sampler,
        num_workers=0,  # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏ –æ—Ç–∫–ª—é—á–∞–µ–º –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å
        pin_memory=False  # –ù–∞ CPU –Ω–µ—Ç —Å–º—ã—Å–ª–∞
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False,
        num_workers=0,  # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏ –æ—Ç–∫–ª—é—á–∞–µ–º –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å
        pin_memory=False  # –ù–∞ CPU –Ω–µ—Ç —Å–º—ã—Å–ª–∞
    )
    
    print(f"   Train batches: {len(train_loader)}")
    print(f"   Valid batches: {len(val_loader)}")
    
    # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–≤–æ–≥–æ –±–∞—Ç—á–∞:")
    try:
        imgs, labels = next(iter(train_loader))
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö
        print(f"   –¢–∏–ø –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {type(imgs)}")
        print(f"   –ê—Ç—Ä–∏–±—É—Ç shape: {getattr(imgs, 'shape', '–ù–ï–¢ –ê–¢–†–ò–ë–£–¢–ê SHAPE!')}")
        
        # –ï—Å–ª–∏ —ç—Ç–æ —Ç–µ–Ω–∑–æ—Ä - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–µ—Ç–∞–ª–∏
        if isinstance(imgs, torch.Tensor):
            assert imgs.ndim == 4 and imgs.size(1) == 3 and imgs.size(2) == 224
            print(f"   ‚úÖ Batch shape: {imgs.shape}")
            print(f"   ‚úÖ Batch dtype: {imgs.dtype}")
            print(f"   ‚úÖ Value range: [{imgs.min().item():.3f}, {imgs.max().item():.3f}]")
            print(f"   ‚úÖ Labels: {labels[:5].tolist()}")
            print("   ‚úÖ –ë–∞—Ç—á –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        else:
            print(f"   ‚ùå –û–®–ò–ë–ö–ê: –û–∂–∏–¥–∞–ª–∏ torch.Tensor, –ø–æ–ª—É—á–∏–ª–∏ {type(imgs)}")
            print("   üîß –ü—Ä–æ–±–ª–µ–º–∞ –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è—Ö - ToTensor() –Ω–µ –ø—Ä–∏–º–µ–Ω–∏–ª—Å—è!")
            return
            
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞—Ç—á–∞: {e}")
        print(f"   üîß –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ!")
        return
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    print("\n4Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏")
    model = MulticlassDamageModel(num_classes=3).to(DEVICE)
    
    # üßÆ –ü–ï–†–ï–°–ß–ò–¢–´–í–ê–ï–ú –≤–µ—Å–∞ –¥–ª—è FocalLoss –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–æ–≤–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    # –ù–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: [1473, 424, 292] ‚Üí –∏–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è –±–∞–ª–∞–Ω—Å–∞
    total_samples = 1473 + 424 + 292  # 2189
    no_damage_weight = total_samples / (3 * 1473)      # 0.496
    minor_damage_weight = total_samples / (3 * 424)    # 1.721  
    major_damage_weight = total_samples / (3 * 292)    # 2.499
    
    print(f"üìä –ù–û–í–´–ï –í–ï–°–ê –ö–õ–ê–°–°–û–í:")
    print(f"   ‚Ä¢ no_damage: {no_damage_weight:.3f}")
    print(f"   ‚Ä¢ minor_damage: {minor_damage_weight:.3f}")
    print(f"   ‚Ä¢ major_damage: {major_damage_weight:.3f}")
    
    # Criterion –∏ optimizer —Å –ü–ï–†–ï–°–ß–ò–¢–ê–ù–ù–´–ú–ò –≤–µ—Å–∞–º–∏ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤
    criterion = FocalLoss(alpha=[no_damage_weight, minor_damage_weight, major_damage_weight], gamma=2.0, device=DEVICE)
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=0.0001,  # üîß –°–ù–ò–ñ–ê–ï–ú —Å 0.001 –¥–æ 0.0001 –¥–ª—è fine-tuning
        weight_decay=5e-4
    )
    
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=10, T_mult=2
    )
    
    print(f"   Device: {DEVICE}")
    print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in model.parameters()):,}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å - –ò–°–ü–†–ê–í–õ–ï–ù –ü–£–¢–¨ –ù–ê –•–û–†–û–®–ò–ô CHECKPOINT
    checkpoint_path = "training_results/best_model.pth"  # –•–û–†–û–®–ò–ô checkpoint —Å F1=0.6020 (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ car_state)
    start_epoch = 0
    best_f1 = 0.0
    
    if os.path.exists(checkpoint_path):
        print(f"\nüîÑ –ó–ê–ì–†–£–ñ–ê–ï–ú –°–û–•–†–ê–ù–ï–ù–ù–£–Æ –ú–û–î–ï–õ–¨: {checkpoint_path}")
        print(f"   üìÅ –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {os.path.getsize(checkpoint_path) / (1024*1024):.2f} MB")
        
        try:
            checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=False)
            print(f"   üì¶ –¢–∏–ø checkpoint: {type(checkpoint)}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç checkpoint
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                # –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
                print(f"   üÜï –û–ë–ù–ê–†–£–ñ–ï–ù –ü–û–õ–ù–´–ô CHECKPOINT –° –ú–ï–¢–ê–î–ê–ù–ù–´–ú–ò")
                print(f"   üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª—é—á–∏: {list(checkpoint.keys())}")
                
                model.load_state_dict(checkpoint['model_state_dict'])
                
                # –ù–ï –∑–∞–≥—Ä—É–∂–∞–µ–º optimizer state –∏–∑-–∑–∞ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                
                start_epoch = checkpoint.get('epoch', 0)
                best_f1 = checkpoint.get('val_f1', checkpoint.get('best_f1', 0.0))
                
                print(f"   ‚úÖ –£–°–ü–ï–®–ù–û –ó–ê–ì–†–£–ñ–ï–ù–ê –ú–û–î–ï–õ–¨:")
                print(f"      üéØ –≠–ø–æ—Ö–∞: {start_epoch}")
                print(f"      üèÜ –õ—É—á—à–∏–π F1-score: {best_f1:.4f}")
                print(f"      ‚ö†Ô∏è Optimizer state –ù–ï –∑–∞–≥—Ä—É–∂–µ–Ω (–Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
                if 'val_metrics' in checkpoint:
                    val_metrics = checkpoint['val_metrics']
                    print(f"   üìä –î–ï–¢–ê–õ–¨–ù–´–ï –ú–ï–¢–†–ò–ö–ò –ú–û–î–ï–õ–ò:")
                    print(f"      ‚Ä¢ Accuracy: {val_metrics.get('accuracy', 'N/A'):.4f}")
                    print(f"      ‚Ä¢ Macro F1: {val_metrics.get('macro_f1', 'N/A'):.4f}")
                    print(f"      ‚Ä¢ Weighted F1: {val_metrics.get('weighted_f1', 'N/A'):.4f}")
                    print(f"      ‚Ä¢ F1 no_damage: {val_metrics.get('f1_no_damage', 'N/A'):.4f}")
                    print(f"      ‚Ä¢ F1 minor_damage: {val_metrics.get('f1_minor_damage', 'N/A'):.4f}")
                    print(f"      ‚Ä¢ F1 major_damage: {val_metrics.get('f1_major_damage', 'N/A'):.4f}")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º optimizer state
                if 'optimizer_state_dict' in checkpoint:
                    current_lr = optimizer.param_groups[0]['lr']
                    print(f"      üîß Learning Rate: {current_lr:.6f}")
                
                print(f"   üöÄ –ü–†–û–î–û–õ–ñ–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï –° –ü–†–ï–î–û–ë–£–ß–ï–ù–ù–´–ú–ò –í–ï–°–ê–ú–ò!")
                
            else:
                # –°—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç - —Ç–æ–ª—å–∫–æ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ (OrderedDict)
                print(f"   üìú –û–ë–ù–ê–†–£–ñ–ï–ù –°–¢–ê–†–´–ô –§–û–†–ú–ê–¢ - –¢–û–õ–¨–ö–û –í–ï–°–ê –ú–û–î–ï–õ–ò")
                print(f"   üìã –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ checkpoint: {len(checkpoint)}")
                
                model.load_state_dict(checkpoint)
                start_epoch = 0
                best_f1 = 0.0
                
                print(f"   ‚úÖ –£–°–ü–ï–®–ù–û –ó–ê–ì–†–£–ñ–ï–ù–´ –í–ï–°–ê –ú–û–î–ï–õ–ò:")
                print(f"      ‚ö†Ô∏è –¢–æ–ª—å–∫–æ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ (–±–µ–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö)")
                print(f"      ‚ö†Ô∏è Epoch –∏ optimizer state —Å–±—Ä–æ—à–µ–Ω—ã")
                print(f"      üîÑ –ù–∞—á–∏–Ω–∞–µ–º —Å —ç–ø–æ—Ö–∏ 1, –Ω–æ —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏!")
                
        except Exception as e:
            print(f"   ‚ùå –û–®–ò–ë–ö–ê –ó–ê–ì–†–£–ó–ö–ò CHECKPOINT:")
            print(f"      –ü—Ä–∏—á–∏–Ω–∞: {e}")
            print(f"   üÜï –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –° –ù–£–õ–Ø (—Å–ª—É—á–∞–π–Ω—ã–µ –≤–µ—Å–∞)")
            start_epoch = 0
            best_f1 = 0.0
    else:
        print(f"\nüÜï –°–û–•–†–ê–ù–ï–ù–ù–ê–Ø –ú–û–î–ï–õ–¨ –ù–ï –ù–ê–ô–î–ï–ù–ê")
        print(f"   üìÅ –ü—É—Ç—å –ø–æ–∏—Å–∫–∞: {checkpoint_path}")
        print(f"   üé≤ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –° –ù–£–õ–Ø (—Å–ª—É—á–∞–π–Ω—ã–µ –≤–µ—Å–∞)")
    
    # –û–±—É—á–µ–Ω–∏–µ
    print(f"\nüöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø")
    print(f"   üìä –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
    print(f"      ‚Ä¢ –°—Ç–∞—Ä—Ç–æ–≤–∞—è —ç–ø–æ—Ö–∞: {start_epoch + 1}")
    print(f"      ‚Ä¢ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö: 30")
    print(f"      ‚Ä¢ –¢–µ–∫—É—â–∏–π –ª—É—á—à–∏–π F1: {best_f1:.4f}")
    print(f"      ‚Ä¢ –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {DEVICE}")
    print(f"      ‚Ä¢ Batch size: {train_loader.batch_size}")
    
    # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –≤—ã–≤–æ–¥ learning rates
    if len(optimizer.param_groups) == 1:
        print(f"      ‚Ä¢ Learning rate: {optimizer.param_groups[0]['lr']:.1e}")
    else:
        print(f"      ‚Ä¢ Learning rates: backbone={optimizer.param_groups[0]['lr']:.1e}, classifier={optimizer.param_groups[1]['lr']:.1e}")
    
    if start_epoch > 0:
        print(f"   üîÑ –†–ï–ñ–ò–ú: –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏")
        print(f"   üéØ –¶–ï–õ–¨: –£–ª—É—á—à–∏—Ç—å F1-score —Å {best_f1:.4f}")
    else:
        if best_f1 > 0.4:  # –ï—Å–ª–∏ F1 > 0.4, –∑–Ω–∞—á–∏—Ç –≤–µ—Å–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω—ã
            print(f"   üîÑ –†–ï–ñ–ò–ú: –î–æ–æ–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏ (F1={best_f1:.4f})")
            print(f"   üéØ –¶–ï–õ–¨: –ü—Ä–µ–≤–∑–æ–π—Ç–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç F1={best_f1:.4f}")
        else:
            print(f"   üÜï –†–ï–ñ–ò–ú: –û–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è (—Å–ª—É—á–∞–π–Ω—ã–µ –≤–µ—Å–∞)")
            print(f"   üéØ –¶–ï–õ–¨: –î–æ—Å—Ç–∏—á—å F1-score > 0.6")
    
    print(f"   ‚è∞ Early stopping: {10} —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è")
    print(f"="*60)
    
    patience = 10
    patience_counter = 0
    
    num_epochs = 30
    
    for epoch in range(start_epoch, num_epochs):
        print(f"\nüìÖ –≠–ø–æ—Ö–∞ {epoch+1}/{num_epochs}")
        if start_epoch > 0 and epoch == start_epoch:
            print(f"   üîÑ –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (F1={best_f1:.4f})")
        print("-" * 30)
        
        # –û–±—É—á–µ–Ω–∏–µ
        train_loss, train_f1 = train_epoch(model, train_loader, criterion, optimizer, DEVICE)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        val_loss, val_f1 = validate_epoch(model, val_loader, criterion, DEVICE)
        
        # Scheduler step
        scheduler.step()
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–ø–æ—Ö–∏
        print(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –≠–ü–û–•–ò {epoch+1}:")
        print(f"   üèãÔ∏è Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}")
        print(f"   üéØ Valid Loss: {val_loss:.4f}, Valid F1: {val_f1:.4f}")
        print(f"   ‚öôÔ∏è Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –ø–µ—Ä–≤–æ–π —ç–ø–æ—Ö–∏
        if epoch == start_epoch:
            print(f"\nüîç –ê–ù–ê–õ–ò–ó –ü–ï–†–í–û–ô –≠–ü–û–•–ò:")
            if val_f1 > 0.4:  # –ï—Å–ª–∏ F1 > 0.4, —Ç–æ —è–≤–Ω–æ –Ω–µ —Å–ª—É—á–∞–π–Ω—ã–µ –≤–µ—Å–∞
                print(f"   ‚úÖ F1={val_f1:.4f} > 0.4 ‚Üí –ü–†–ï–î–û–ë–£–ß–ï–ù–ù–´–ï –í–ï–°–ê –ó–ê–ì–†–£–ñ–ï–ù–´ –£–°–ü–ï–®–ù–û!")
                print(f"   üéâ –ú–æ–¥–µ–ª—å –Ω–∞—á–∏–Ω–∞–µ—Ç —Å —Ö–æ—Ä–æ—à–µ–≥–æ —É—Ä–æ–≤–Ω—è, –∞ –Ω–µ —Å –Ω—É–ª—è")
            elif val_f1 > 0.2:
                print(f"   ‚ö†Ô∏è F1={val_f1:.4f} > 0.2 ‚Üí –í–æ–∑–º–æ–∂–Ω–æ —á–∞—Å—Ç–∏—á–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤–µ—Å–∞")
                print(f"   ü§î –ò–ª–∏ –ø—Ä–æ—Å—Ç–æ —É–¥–∞—á–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è")
            else:
                print(f"   ‚ùå F1={val_f1:.4f} < 0.2 ‚Üí –ü–û–•–û–ñ–ï –ù–ê –°–õ–£–ß–ê–ô–ù–´–ï –í–ï–°–ê!")
                print(f"   üö® Checkpoint –º–æ–≥ –Ω–µ –∑–∞–≥—Ä—É–∑–∏—Ç—å—Å—è –∏–ª–∏ —Å–±—Ä–æ—Å–∏—Ç—å—Å—è")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        if val_f1 > best_f1:
            improvement = val_f1 - best_f1
            best_f1 = val_f1
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π checkpoint —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_f1': best_f1,
                'val_metrics': {
                    'accuracy': train_f1,  # –ó–¥–µ—Å—å –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                    'macro_f1': val_f1,
                    'train_f1': train_f1,
                    'val_loss': val_loss,
                    'train_loss': train_loss
                }
            }, 'training_results/best_model.pth')
            
            print(f"\nüèÜ –ù–û–í–ê–Ø –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨!")
            print(f"   üìà F1-score: {best_f1:.4f} (+{improvement:.4f})")
            print(f"   üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: training_results/best_model.pth")
            patience_counter = 0
        else:
            decline = best_f1 - val_f1
            patience_counter += 1
            print(f"\nüìâ –ë–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è: F1={val_f1:.4f} (–ª—É—á—à–∏–π: {best_f1:.4f}, -{decline:.4f})")
            print(f"   ‚è∞ Patience: {patience_counter}/{patience}")
            
        if patience_counter >= patience:
            print(f"\n‚è∞ EARLY STOPPING –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
            print(f"   üõë {patience} —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è")
            print(f"   üèÜ –§–∏–Ω–∞–ª—å–Ω—ã–π –ª—É—á—à–∏–π F1: {best_f1:.4f}")
            break
    
    print(f"\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"   –õ—É—á—à–∏–π F1-score: {best_f1:.4f}")
    print(f"   –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: ../best_multiclass_model_v3.2.pth")

if __name__ == "__main__":
    main()