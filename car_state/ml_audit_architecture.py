"""
ML Engineering Audit - –ê–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏
–î–µ—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ MulticlassDamageModel –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º –æ–±—É—á–µ–Ω–∏—è
"""
import torch
import torch.nn as nn
from multiclass_damage_model import MulticlassDamageModel, FocalLoss, create_training_transforms, create_validation_transforms

def analyze_model_architecture():
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–∏"""
    
    print("üîç ML ENGINEERING AUDIT - –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ú–û–î–ï–õ–ò")
    print("="*60)
    
    # 1. –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    print("\n1Ô∏è‚É£ –ê–ù–ê–õ–ò–ó –ü–ê–†–ê–ú–ï–¢–†–û–í –ò –°–¢–†–£–ö–¢–£–†–´")
    model = MulticlassDamageModel(num_classes=3)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    backbone_params = sum(p.numel() for p in model.backbone.parameters())
    classifier_params = sum(p.numel() for p in model.classifier.parameters())
    
    print(f"   üìä –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
    print(f"   üéØ –û–±—É—á–∞–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {trainable_params:,}")
    print(f"   üèóÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã backbone (ResNet50): {backbone_params:,}")
    print(f"   üß† –ü–∞—Ä–∞–º–µ—Ç—Ä—ã classifier: {classifier_params:,}")
    print(f"   üí° –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ classifier/total: {classifier_params/total_params*100:.1f}%")
    
    # 2. –ê–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    print("\n2Ô∏è‚É£ –ê–ù–ê–õ–ò–ó –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–†–ê")
    print("   üîß –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–ª–æ–µ–≤:")
    for i, layer in enumerate(model.classifier):
        if isinstance(layer, nn.Linear):
            print(f"      Linear {i}: {layer.in_features} ‚Üí {layer.out_features}")
        elif isinstance(layer, nn.Dropout):
            print(f"      Dropout {i}: p={layer.p}")
        elif isinstance(layer, nn.BatchNorm1d):
            print(f"      BatchNorm1d {i}: {layer.num_features} features")
        else:
            print(f"      {type(layer).__name__} {i}")
    
    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ dropout progression
    dropout_layers = [layer for layer in model.classifier if isinstance(layer, nn.Dropout)]
    print(f"\n   üìâ Dropout progression:")
    for i, layer in enumerate(dropout_layers):
        print(f"      Dropout {i+1}: {layer.p}")
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ dropout
    if len(dropout_layers) >= 3:
        rates = [layer.p for layer in dropout_layers]
        if rates == sorted(rates, reverse=True):
            print("   ‚úÖ Dropout rates —É–±—ã–≤–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        else:
            print("   ‚ö†Ô∏è –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–±—ã–≤–∞—é—â–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å dropout rates")
    
    # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π
    print("\n3Ô∏è‚É£ –ü–†–û–í–ï–†–ö–ê FORWARD PASS")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã batch
    batch_sizes = [1, 4, 16, 32]
    for bs in batch_sizes:
        try:
            dummy_input = torch.randn(bs, 3, 224, 224)
            with torch.no_grad():
                output = model(dummy_input)
            expected_shape = (bs, 3)
            
            if output.shape == expected_shape:
                print(f"   ‚úÖ Batch size {bs}: {output.shape} ‚úì")
            else:
                print(f"   ‚ùå Batch size {bs}: {output.shape} ‚â† {expected_shape}")
                
        except Exception as e:
            print(f"   ‚ùå Batch size {bs}: ERROR - {e}")
    
    # 5. –ê–Ω–∞–ª–∏–∑ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    print("\n4Ô∏è‚É£ –ê–ù–ê–õ–ò–ó –ì–†–ê–î–ò–ï–ù–¢–ù–û–ì–û –ü–û–¢–û–ö–ê")
    
    model.train()
    dummy_input = torch.randn(2, 3, 224, 224)
    dummy_target = torch.randint(0, 3, (2,))
    criterion = nn.CrossEntropyLoss()
    
    output = model(dummy_input)
    loss = criterion(output, dummy_target)
    loss.backward()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤ —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –º–æ–¥–µ–ª–∏
    backbone_grads = []
    classifier_grads = []
    
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            if 'backbone' in name:
                backbone_grads.append(grad_norm)
            elif 'classifier' in name:
                classifier_grads.append(grad_norm)
    
    print(f"   üèóÔ∏è –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã backbone - —Å—Ä–µ–¥–Ω–µ–µ: {sum(backbone_grads)/len(backbone_grads):.6f}")
    print(f"   üß† –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã classifier - —Å—Ä–µ–¥–Ω–µ–µ: {sum(classifier_grads)/len(classifier_grads):.6f}")
    
    if len(backbone_grads) > 0 and len(classifier_grads) > 0:
        ratio = (sum(classifier_grads)/len(classifier_grads)) / (sum(backbone_grads)/len(backbone_grads))
        print(f"   üìä –û—Ç–Ω–æ—à–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ classifier/backbone: {ratio:.2f}")
        
        if 10 <= ratio <= 1000:
            print("   ‚úÖ –•–æ—Ä–æ—à–µ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤")
        elif ratio > 1000:
            print("   ‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤ classifier - –≤–æ–∑–º–æ–∂–µ–Ω exploding gradient")
        else:
            print("   ‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –≤ classifier - –≤–æ–∑–º–æ–∂–µ–Ω vanishing gradient")
    
    # 6. –ê–Ω–∞–ª–∏–∑ –∞–∫—Ç–∏–≤–∞—Ü–∏–π
    print("\n5Ô∏è‚É£ –ê–ù–ê–õ–ò–ó –ê–ö–¢–ò–í–ê–¶–ò–ô")
    model.eval()
    
    with torch.no_grad():
        dummy_input = torch.randn(4, 3, 224, 224)
        
        # –ü—Ä–æ—Ö–æ–¥–∏–º —á–µ—Ä–µ–∑ backbone
        x = model.backbone.conv1(dummy_input)
        print(f"   –ü–æ—Å–ª–µ conv1: {x.shape}, —Å—Ä–µ–¥–Ω–µ–µ: {x.mean():.4f}, std: {x.std():.4f}")
        
        x = model.backbone.bn1(x)
        x = model.backbone.relu(x)
        x = model.backbone.maxpool(x)
        
        x = model.backbone.layer1(x)
        print(f"   –ü–æ—Å–ª–µ layer1: {x.shape}, —Å—Ä–µ–¥–Ω–µ–µ: {x.mean():.4f}, std: {x.std():.4f}")
        
        x = model.backbone.layer2(x)
        print(f"   –ü–æ—Å–ª–µ layer2: {x.shape}, —Å—Ä–µ–¥–Ω–µ–µ: {x.mean():.4f}, std: {x.std():.4f}")
        
        x = model.backbone.layer3(x)
        print(f"   –ü–æ—Å–ª–µ layer3: {x.shape}, —Å—Ä–µ–¥–Ω–µ–µ: {x.mean():.4f}, std: {x.std():.4f}")
        
        x = model.backbone.layer4(x)
        print(f"   –ü–æ—Å–ª–µ layer4: {x.shape}, —Å—Ä–µ–¥–Ω–µ–µ: {x.mean():.4f}, std: {x.std():.4f}")
        
        x = model.backbone.avgpool(x)
        x = torch.flatten(x, 1)
        print(f"   –ü–æ—Å–ª–µ avgpool+flatten: {x.shape}, —Å—Ä–µ–¥–Ω–µ–µ: {x.mean():.4f}, std: {x.std():.4f}")
        
        # –ü—Ä–æ—Ö–æ–¥–∏–º —á–µ—Ä–µ–∑ classifier
        for i, layer in enumerate(model.classifier):
            x = layer(x)
            if isinstance(layer, (nn.Linear, nn.ReLU, nn.BatchNorm1d)):
                print(f"   –ü–æ—Å–ª–µ classifier[{i}] ({type(layer).__name__}): —Å—Ä–µ–¥–Ω–µ–µ: {x.mean():.4f}, std: {x.std():.4f}")
    
    return model

def analyze_focal_loss():
    """–ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ Focal Loss"""
    
    print("\n6Ô∏è‚É£ –ê–ù–ê–õ–ò–ó FOCAL LOSS")
    print("   üéØ –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ gamma:")
    
    # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º
    logits = torch.randn(100, 3)  # 100 —Å–µ–º–ø–ª–æ–≤, 3 –∫–ª–∞—Å—Å–∞
    
    # –ò–º–∏—Ç–∏—Ä—É–µ–º –¥–∏—Å–±–∞–ª–∞–Ω—Å: –º–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ 2, –º–∞–ª–æ –∫–ª–∞—Å—Å–∞ 0
    targets = torch.cat([
        torch.zeros(10, dtype=torch.long),    # 10% –∫–ª–∞—Å—Å 0 (no_damage)
        torch.ones(40, dtype=torch.long),     # 40% –∫–ª–∞—Å—Å 1 (minor)
        torch.full((50,), 2, dtype=torch.long) # 50% –∫–ª–∞—Å—Å 2 (major)
    ])
    
    # –í—ã—á–∏—Å–ª—è–µ–º class weights
    class_counts = torch.bincount(targets).float()
    total = class_counts.sum()
    class_weights = total / (len(class_counts) * class_counts)
    
    print(f"   üìä –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {class_counts.numpy()}")
    print(f"   ‚öñÔ∏è Class weights: {class_weights.numpy()}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ gamma
    gammas = [0.0, 1.0, 2.0, 3.0]
    ce_loss = nn.CrossEntropyLoss(weight=class_weights)
    
    for gamma in gammas:
        focal_loss = FocalLoss(alpha=class_weights, gamma=gamma)
        
        ce_val = ce_loss(logits, targets).item()
        focal_val = focal_loss(logits, targets).item()
        
        print(f"   Gamma {gamma}: CE={ce_val:.4f}, Focal={focal_val:.4f}, ratio={focal_val/ce_val:.3f}")
    
    print("\n   üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
    print("   - Gamma=2.0 —Ö–æ—Ä–æ—à–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ 8:1")
    print("   - Class weights –∫–æ–º–ø–µ–Ω—Å–∏—Ä—É—é—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å")
    print("   - Focal Loss —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ hard examples")

def analyze_transforms():
    """–ê–Ω–∞–ª–∏–∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π –¥–∞–Ω–Ω—ã—Ö"""
    
    print("\n7Ô∏è‚É£ –ê–ù–ê–õ–ò–ó –¢–†–ê–ù–°–§–û–†–ú–ê–¶–ò–ô")
    
    train_transforms = create_training_transforms()
    val_transforms = create_validation_transforms()
    
    print("   üèãÔ∏è Training transforms:")
    for i, transform in enumerate(train_transforms.transforms):
        print(f"      {i+1}. {transform}")
    
    print("\n   üîç Validation transforms:")
    for i, transform in enumerate(val_transforms.transforms):
        print(f"      {i+1}. {transform}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    from PIL import Image
    import numpy as np
    
    # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    dummy_image = Image.fromarray(np.random.randint(0, 255, (300, 400, 3), dtype=np.uint8))
    
    try:
        train_tensor = train_transforms(dummy_image)
        val_tensor = val_transforms(dummy_image)
        
        print(f"\n   üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–π:")
        print(f"      Training tensor: {train_tensor.shape}, range: [{train_tensor.min():.3f}, {train_tensor.max():.3f}]")
        print(f"      Validation tensor: {val_tensor.shape}, range: [{val_tensor.min():.3f}, {val_tensor.max():.3f}]")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é ImageNet
        mean = train_tensor.mean(dim=[1, 2])
        std = train_tensor.std(dim=[1, 2])
        print(f"      Training –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è - mean: {mean.numpy()}, std: {std.numpy()}")
        
        print("\n   ‚úÖ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        
    except Exception as e:
        print(f"   ‚ùå –û—à–∏–±–∫–∞ –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è—Ö: {e}")
    
    print("\n   üí° –û—Ü–µ–Ω–∫–∞ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π –¥–ª—è car damage:")
    print("   ‚úÖ RandomHorizontalFlip - —Ö–æ—Ä–æ—à–æ (–ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏—è —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã)")
    print("   ‚úÖ RandomRotation(10¬∞) - –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ (–Ω–µ –¥–æ–ª–∂–Ω—ã –ø–µ—Ä–µ–≤–æ—Ä–∞—á–∏–≤–∞—Ç—å –º–∞—à–∏–Ω—É)")
    print("   ‚úÖ ColorJitter - —Ö–æ—Ä–æ—à–æ (—Ä–∞–∑–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –æ—Å–≤–µ—â–µ–Ω–∏—è)")
    print("   ‚úÖ RandomResizedCrop - —Ö–æ—Ä–æ—à–æ (—Ä–∞–∑–Ω—ã–µ —Ä–∞–∫—É—Ä—Å—ã –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π)")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞"""
    
    model = analyze_model_architecture()
    analyze_focal_loss()
    analyze_transforms()
    
    print("\nüéØ –ò–¢–û–ì–û–í–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    print("="*60)
    print("‚úÖ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è 3-class –∑–∞–¥–∞—á–∏")
    print("‚úÖ ResNet50 backbone - —Ö–æ—Ä–æ—à–∏–π –≤—ã–±–æ—Ä –¥–ª—è car damage detection")
    print("‚úÖ Dropout progression –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞ (0.5 ‚Üí 0.25 ‚Üí 0.125)")
    print("‚úÖ BatchNorm –º–µ–∂–¥—É Linear —Å–ª–æ—è–º–∏ –ø–æ–º–æ–∂–µ—Ç —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏")
    print("‚úÖ FocalLoss —Å gamma=2.0 —Ö–æ—Ä–æ—à –¥–ª—è –¥–∏—Å–±–∞–ª–∞–Ω—Å–∞ 8:1")
    print("‚úÖ Class weights –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–æ–º–ø–µ–Ω—Å–∏—Ä—É—é—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å")
    print("‚úÖ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è car damage detection")
    
    print("\n‚ö†Ô∏è –ü–û–¢–ï–ù–¶–ò–ê–õ–¨–ù–´–ï –†–ò–°–ö–ò:")
    print("‚ùó –û—á–µ–Ω—å –º–∞–ª—ã–π –∫–ª–∞—Å—Å no_damage (6.3%) - –≤–æ–∑–º–æ–∂–Ω—ã false negatives")
    print("‚ùó CPU –æ–±—É—á–µ–Ω–∏–µ –±—É–¥–µ—Ç –º–µ–¥–ª–µ–Ω–Ω—ã–º (~23M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)")
    print("‚ùó –¢—Ä–µ–±—É–µ—Ç—Å—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ overfitting –Ω–∞ –º–∞–ª–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ (650 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)")
    print("‚ùó –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å –¥–æ–ª—é validation –¥–∞–Ω–Ω—ã—Ö –¥–æ 25-30%")
    
    print(f"\nüìä –†–ê–ó–ú–ï–† –ú–û–î–ï–õ–ò:")
    total_params = sum(p.numel() for p in model.parameters())
    model_size_mb = total_params * 4 / (1024 * 1024)  # float32 = 4 bytes
    print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {total_params:,}")
    print(f"   –†–∞–∑–º–µ—Ä –≤ –ø–∞–º—è—Ç–∏: ~{model_size_mb:.1f} MB")
    print(f"   –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ CPU: 2-4 —á–∞—Å–∞ –¥–ª—è 20 —ç–ø–æ—Ö")

if __name__ == "__main__":
    main()